<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Capítulo 2 Revisão | Econometria e Séries Temporais com R</title>
  <meta name="description" content="Este livro pode ser usado em um curso de graduação que proponha estudar econometria e séreis temporais em um nível introdutório. No entanto, também é abordado um pouco da teoria e tecnicalidade." />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="Capítulo 2 Revisão | Econometria e Séries Temporais com R" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Este livro pode ser usado em um curso de graduação que proponha estudar econometria e séreis temporais em um nível introdutório. No entanto, também é abordado um pouco da teoria e tecnicalidade." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Capítulo 2 Revisão | Econometria e Séries Temporais com R" />
  
  <meta name="twitter:description" content="Este livro pode ser usado em um curso de graduação que proponha estudar econometria e séreis temporais em um nível introdutório. No entanto, também é abordado um pouco da teoria e tecnicalidade." />
  

<meta name="author" content="Marcio Valk e Guilherme Pumi" />


<meta name="date" content="2020-07-12" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="intro.html"/>
<link rel="next" href="streg.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Minimal Book Example</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Prefácio</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Breve introdução ao R</a><ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#apresentação-da-linguagem-r"><i class="fa fa-check"></i><b>1.1</b> Apresentação da linguagem R</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#instalando-o-r"><i class="fa fa-check"></i><b>1.2</b> Instalando o R</a></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#instalando-o-rstudio"><i class="fa fa-check"></i><b>1.3</b> Instalando o RStudio</a></li>
<li class="chapter" data-level="1.4" data-path="intro.html"><a href="intro.html#diretório-de-trabalho"><i class="fa fa-check"></i><b>1.4</b> Diretório de trabalho</a></li>
<li class="chapter" data-level="1.5" data-path="intro.html"><a href="intro.html#rstudio-cloud"><i class="fa fa-check"></i><b>1.5</b> RStudio Cloud</a></li>
<li class="chapter" data-level="1.6" data-path="intro.html"><a href="intro.html#instalação-de-pacotes"><i class="fa fa-check"></i><b>1.6</b> Instalação de pacotes</a></li>
<li class="chapter" data-level="1.7" data-path="intro.html"><a href="intro.html#funcionalidades-básicas"><i class="fa fa-check"></i><b>1.7</b> Funcionalidades básicas</a><ul>
<li class="chapter" data-level="1.7.1" data-path="intro.html"><a href="intro.html#operações"><i class="fa fa-check"></i><b>1.7.1</b> Operações</a></li>
<li class="chapter" data-level="1.7.2" data-path="intro.html"><a href="intro.html#operadores-lógicos"><i class="fa fa-check"></i><b>1.7.2</b> Operadores lógicos</a></li>
</ul></li>
<li class="chapter" data-level="1.8" data-path="intro.html"><a href="intro.html#variáveis-no-r"><i class="fa fa-check"></i><b>1.8</b> Variáveis no R</a><ul>
<li class="chapter" data-level="1.8.1" data-path="intro.html"><a href="intro.html#operações-1"><i class="fa fa-check"></i><b>1.8.1</b> Operações</a></li>
</ul></li>
<li class="chapter" data-level="1.9" data-path="intro.html"><a href="intro.html#vetores"><i class="fa fa-check"></i><b>1.9</b> Vetores</a><ul>
<li class="chapter" data-level="1.9.1" data-path="intro.html"><a href="intro.html#operações-com-vetores"><i class="fa fa-check"></i><b>1.9.1</b> Operações com vetores</a></li>
</ul></li>
<li class="chapter" data-level="1.10" data-path="intro.html"><a href="intro.html#matrizes"><i class="fa fa-check"></i><b>1.10</b> Matrizes</a><ul>
<li class="chapter" data-level="1.10.1" data-path="intro.html"><a href="intro.html#operações-com-matrizes"><i class="fa fa-check"></i><b>1.10.1</b> Operações com Matrizes</a></li>
</ul></li>
<li class="chapter" data-level="1.11" data-path="intro.html"><a href="intro.html#uso-do-for-loop"><i class="fa fa-check"></i><b>1.11</b> Uso do for (loop)</a></li>
<li class="chapter" data-level="1.12" data-path="intro.html"><a href="intro.html#funções"><i class="fa fa-check"></i><b>1.12</b> Funções</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="rev.html"><a href="rev.html"><i class="fa fa-check"></i><b>2</b> Revisão</a><ul>
<li class="chapter" data-level="2.1" data-path="rev.html"><a href="rev.html#aleatoriedade-a-essência-da-estatística"><i class="fa fa-check"></i><b>2.1</b> Aleatoriedade, a essência da estatística</a></li>
<li class="chapter" data-level="2.2" data-path="rev.html"><a href="rev.html#variável-aleatória"><i class="fa fa-check"></i><b>2.2</b> Variável aleatória</a><ul>
<li class="chapter" data-level="2.2.1" data-path="rev.html"><a href="rev.html#distribuição-de-probabilidade"><i class="fa fa-check"></i><b>2.2.1</b> Distribuição de probabilidade</a></li>
<li class="chapter" data-level="2.2.2" data-path="rev.html"><a href="rev.html#distribuições-conjunta-marginal-e-condicional"><i class="fa fa-check"></i><b>2.2.2</b> Distribuições conjunta, marginal e condicional</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="rev.html"><a href="rev.html#a-distribuição-normal-e-distribuições-relacionadas"><i class="fa fa-check"></i><b>2.3</b> A distribuição Normal e distribuições relacionadas</a><ul>
<li class="chapter" data-level="2.3.1" data-path="rev.html"><a href="rev.html#a-distribuição-normal"><i class="fa fa-check"></i><b>2.3.1</b> A distribuição Normal</a></li>
<li class="chapter" data-level="2.3.2" data-path="rev.html"><a href="rev.html#distribuições-relacionadas"><i class="fa fa-check"></i><b>2.3.2</b> Distribuições relacionadas</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="rev.html"><a href="rev.html#parâmetros-estimadores-e-valores-estimados"><i class="fa fa-check"></i><b>2.4</b> Parâmetros, estimadores e valores estimados</a></li>
<li class="chapter" data-level="2.5" data-path="rev.html"><a href="rev.html#propriedades-de-variáveis-aleatórias"><i class="fa fa-check"></i><b>2.5</b> Propriedades de variáveis aleatórias</a><ul>
<li class="chapter" data-level="2.5.1" data-path="rev.html"><a href="rev.html#média-valor-esperado-ou-esperança-matemática"><i class="fa fa-check"></i><b>2.5.1</b> Média, valor esperado ou esperança matemática</a></li>
<li class="chapter" data-level="2.5.2" data-path="rev.html"><a href="rev.html#propriedades-da-esperança"><i class="fa fa-check"></i><b>2.5.2</b> Propriedades da Esperança</a></li>
<li class="chapter" data-level="2.5.3" data-path="rev.html"><a href="rev.html#variância"><i class="fa fa-check"></i><b>2.5.3</b> Variância</a></li>
<li class="chapter" data-level="2.5.4" data-path="rev.html"><a href="rev.html#covariância"><i class="fa fa-check"></i><b>2.5.4</b> Covariância</a></li>
<li class="chapter" data-level="2.5.5" data-path="rev.html"><a href="rev.html#correlação"><i class="fa fa-check"></i><b>2.5.5</b> Correlação</a></li>
<li class="chapter" data-level="2.5.6" data-path="rev.html"><a href="rev.html#propriedades-da-variância-covariância-e-correlação"><i class="fa fa-check"></i><b>2.5.6</b> Propriedades da variância, covariância e correlação</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="rev.html"><a href="rev.html#estimadores"><i class="fa fa-check"></i><b>2.6</b> Estimadores</a><ul>
<li class="chapter" data-level="2.6.1" data-path="rev.html"><a href="rev.html#propriedades-dos-estimadores"><i class="fa fa-check"></i><b>2.6.1</b> Propriedades dos estimadores</a></li>
<li class="chapter" data-level="2.6.2" data-path="rev.html"><a href="rev.html#vícioviés"><i class="fa fa-check"></i><b>2.6.2</b> Vício/Viés</a></li>
<li class="chapter" data-level="2.6.3" data-path="rev.html"><a href="rev.html#consistência"><i class="fa fa-check"></i><b>2.6.3</b> Consistência</a></li>
<li class="chapter" data-level="2.6.4" data-path="rev.html"><a href="rev.html#eficiência"><i class="fa fa-check"></i><b>2.6.4</b> Eficiência</a></li>
<li class="chapter" data-level="2.6.5" data-path="rev.html"><a href="rev.html#erro-quadrático-médio-eqm"><i class="fa fa-check"></i><b>2.6.5</b> Erro quadrático médio (EQM)</a></li>
<li class="chapter" data-level="2.6.6" data-path="rev.html"><a href="rev.html#vício-versus-vvariância-mínima"><i class="fa fa-check"></i><b>2.6.6</b> Vício versus Vvariância mínima</a></li>
<li class="chapter" data-level="2.6.7" data-path="rev.html"><a href="rev.html#método-de-mínimos-quadrados-mqo"><i class="fa fa-check"></i><b>2.6.7</b> Método de mínimos quadrados (MQO)</a></li>
<li class="chapter" data-level="2.6.8" data-path="rev.html"><a href="rev.html#regressão-liner-múltipla-rml"><i class="fa fa-check"></i><b>2.6.8</b> Regressão liner múltipla (RML)</a></li>
<li class="chapter" data-level="2.6.9" data-path="rev.html"><a href="rev.html#hipóteses-do-modelo-de-regressão"><i class="fa fa-check"></i><b>2.6.9</b> Hipóteses do modelo de regressão</a></li>
<li class="chapter" data-level="2.6.10" data-path="rev.html"><a href="rev.html#o-coeficiente-de-dterminação"><i class="fa fa-check"></i><b>2.6.10</b> O coeficiente de dterminação</a></li>
<li class="chapter" data-level="2.6.11" data-path="rev.html"><a href="rev.html#propriedade-de-não-viés-dos-estimadores-mqo"><i class="fa fa-check"></i><b>2.6.11</b> Propriedade de não-viés dos estimadores MQO</a></li>
<li class="chapter" data-level="2.6.12" data-path="rev.html"><a href="rev.html#testes-de-hipóteses"><i class="fa fa-check"></i><b>2.6.12</b> Testes de hipóteses</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="rev.html"><a href="rev.html#formas-funcionais-logarítmicas"><i class="fa fa-check"></i><b>2.7</b> Formas funcionais logarítmicas</a></li>
<li class="chapter" data-level="2.8" data-path="rev.html"><a href="rev.html#exercícios"><i class="fa fa-check"></i><b>2.8</b> Exercícios</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="streg.html"><a href="streg.html"><i class="fa fa-check"></i><b>3</b> Séries temporais no contexto de regressão</a><ul>
<li class="chapter" data-level="3.1" data-path="streg.html"><a href="streg.html#introdução"><i class="fa fa-check"></i><b>3.1</b> Introdução</a></li>
<li class="chapter" data-level="3.2" data-path="streg.html"><a href="streg.html#exemplos-de-séries-temporais"><i class="fa fa-check"></i><b>3.2</b> Exemplos de séries temporais</a></li>
<li class="chapter" data-level="3.3" data-path="streg.html"><a href="streg.html#regressão-com-dados-de-séries-temporais"><i class="fa fa-check"></i><b>3.3</b> Regressão com dados de séries temporais</a><ul>
<li class="chapter" data-level="3.3.1" data-path="streg.html"><a href="streg.html#diferença-entre-dados-de-séries-temporais-e-dados-de-corte-transversal"><i class="fa fa-check"></i><b>3.3.1</b> Diferença entre dados de séries temporais e dados de corte transversal</a></li>
<li class="chapter" data-level="3.3.2" data-path="streg.html"><a href="streg.html#modelos-de-regressão-de-séries-temporais"><i class="fa fa-check"></i><b>3.3.2</b> Modelos de regressão de séries temporais</a></li>
<li class="chapter" data-level="3.3.3" data-path="streg.html"><a href="streg.html#modelos-de-defasagem-distribuída-finita"><i class="fa fa-check"></i><b>3.3.3</b> Modelos de defasagem distribuída finita</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="streg.html"><a href="streg.html#suposições-para-modelos-com-séries-temporais"><i class="fa fa-check"></i><b>3.4</b> Suposições para modelos com séries temporais</a><ul>
<li class="chapter" data-level="3.4.1" data-path="streg.html"><a href="streg.html#inexistência-de-viés-do-mqo"><i class="fa fa-check"></i><b>3.4.1</b> Inexistência de viés do MQO</a></li>
<li class="chapter" data-level="3.4.2" data-path="rev.html"><a href="rev.html#variância-dos-estimadores-mqo"><i class="fa fa-check"></i><b>3.4.2</b> Variância dos estimadores MQO</a></li>
<li class="chapter" data-level="3.4.3" data-path="streg.html"><a href="streg.html#inferência-sob-as-hipóteses-do-modelo-linear-clássico"><i class="fa fa-check"></i><b>3.4.3</b> Inferência sob as hipóteses do modelo linear clássico</a></li>
<li class="chapter" data-level="3.4.4" data-path="streg.html"><a href="streg.html#tendência"><i class="fa fa-check"></i><b>3.4.4</b> Tendência</a></li>
<li class="chapter" data-level="3.4.5" data-path="streg.html"><a href="streg.html#usando-variáveis-de-tendência-na-análise-de-regressão"><i class="fa fa-check"></i><b>3.4.5</b> Usando variáveis de tendência na análise de regressão</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="streg.html"><a href="streg.html#sazonalidade"><i class="fa fa-check"></i><b>3.5</b> Sazonalidade</a><ul>
<li class="chapter" data-level="3.5.1" data-path="streg.html"><a href="streg.html#processos-de-covariância-estacionária"><i class="fa fa-check"></i><b>3.5.1</b> Processos de covariância estacionária</a></li>
<li class="chapter" data-level="3.5.2" data-path="streg.html"><a href="streg.html#processos-fracamente-dependente"><i class="fa fa-check"></i><b>3.5.2</b> Processos fracamente dependente</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="rev.html"><a href="rev.html#exercícios"><i class="fa fa-check"></i><b>3.6</b> Exercícios</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="st.html"><a href="st.html"><i class="fa fa-check"></i><b>4</b> Séries Temporais</a><ul>
<li class="chapter" data-level="4.1" data-path="st.html"><a href="st.html#objetivos"><i class="fa fa-check"></i><b>4.1</b> Objetivos</a></li>
<li class="chapter" data-level="4.2" data-path="st.html"><a href="st.html#séries-temporais-definição-formal"><i class="fa fa-check"></i><b>4.2</b> Séries temporais: definição formal</a><ul>
<li class="chapter" data-level="4.2.1" data-path="st.html"><a href="st.html#processos-estocásticos"><i class="fa fa-check"></i><b>4.2.1</b> Processos estocásticos</a></li>
<li class="chapter" data-level="4.2.2" data-path="st.html"><a href="st.html#especificação-de-um-processo-estocástico"><i class="fa fa-check"></i><b>4.2.2</b> Especificação de um processo estocástico</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="st.html"><a href="st.html#médias-e-covariâncias"><i class="fa fa-check"></i><b>4.3</b> Médias e covariâncias</a><ul>
<li class="chapter" data-level="4.3.1" data-path="st.html"><a href="st.html#propriedades-importantes"><i class="fa fa-check"></i><b>4.3.1</b> Propriedades importantes</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="st.html"><a href="st.html#estacionariedade"><i class="fa fa-check"></i><b>4.4</b> Estacionariedade</a><ul>
<li class="chapter" data-level="4.4.1" data-path="st.html"><a href="st.html#estacionariedade-forte-ou-estrita"><i class="fa fa-check"></i><b>4.4.1</b> Estacionariedade forte ou estrita</a></li>
<li class="chapter" data-level="4.4.2" data-path="st.html"><a href="st.html#estacionariedade-fraca-ou-de-segunda-ordem"><i class="fa fa-check"></i><b>4.4.2</b> Estacionariedade fraca ou de segunda ordem</a></li>
<li class="chapter" data-level="4.4.3" data-path="st.html"><a href="st.html#teste-para-significância-das-autocorrelações"><i class="fa fa-check"></i><b>4.4.3</b> Teste para significância das autocorrelações</a></li>
<li class="chapter" data-level="4.4.4" data-path="st.html"><a href="st.html#função-de-autocorrelação-parcial-facp"><i class="fa fa-check"></i><b>4.4.4</b> Função de autocorrelação parcial ( <em>FACP</em> )</a></li>
<li class="chapter" data-level="4.4.5" data-path="st.html"><a href="st.html#operador-de-defasagem-ou-operador-lag"><i class="fa fa-check"></i><b>4.4.5</b> Operador de defasagem ou operador <em>lag</em></a></li>
<li class="chapter" data-level="4.4.6" data-path="st.html"><a href="st.html#ruído-branco"><i class="fa fa-check"></i><b>4.4.6</b> Ruído branco</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="st.html"><a href="st.html#metodologia-de-box-jenkins-ou-modelagem-arima"><i class="fa fa-check"></i><b>4.5</b> Metodologia de Box-Jenkins ou modelagem ARIMA</a><ul>
<li class="chapter" data-level="4.5.1" data-path="st.html"><a href="st.html#modelo-autorregressivo-de-ordem-1-ar1"><i class="fa fa-check"></i><b>4.5.1</b> Modelo autorregressivo de ordem 1 AR(1)</a></li>
<li class="chapter" data-level="4.5.2" data-path="st.html"><a href="st.html#passeio-aleatório-random-walk"><i class="fa fa-check"></i><b>4.5.2</b> Passeio aleatório (<em>Random Walk</em>)</a></li>
<li class="chapter" data-level="4.5.3" data-path="st.html"><a href="st.html#modelos-autorregressivos-de-ordem-p-arp"><i class="fa fa-check"></i><b>4.5.3</b> Modelos autorregressivos de ordem <span class="math inline">\(p\)</span>, AR(<span class="math inline">\(p\)</span>)</a></li>
<li class="chapter" data-level="4.5.4" data-path="st.html"><a href="st.html#modelo-de-médias-móveis-maq"><i class="fa fa-check"></i><b>4.5.4</b> Modelo de médias-móveis, MA(<em>q</em>)</a></li>
<li class="chapter" data-level="4.5.5" data-path="st.html"><a href="st.html#o-modelo-ma1"><i class="fa fa-check"></i><b>4.5.5</b> O modelo MA(1)</a></li>
<li class="chapter" data-level="4.5.6" data-path="st.html"><a href="st.html#propriedades-do-modelo-maq"><i class="fa fa-check"></i><b>4.5.6</b> Propriedades do modelo MA(<span class="math inline">\(q\)</span>)</a></li>
<li class="chapter" data-level="4.5.7" data-path="st.html"><a href="st.html#modelo-armapq"><i class="fa fa-check"></i><b>4.5.7</b> Modelo ARMA(<span class="math inline">\(p\)</span>,<span class="math inline">\(q\)</span>)</a></li>
<li class="chapter" data-level="4.5.8" data-path="st.html"><a href="st.html#causalidade"><i class="fa fa-check"></i><b>4.5.8</b> Causalidade</a></li>
<li class="chapter" data-level="4.5.9" data-path="st.html"><a href="st.html#invertibilidade"><i class="fa fa-check"></i><b>4.5.9</b> Invertibilidade</a></li>
<li class="chapter" data-level="4.5.10" data-path="st.html"><a href="st.html#polinômio-característico"><i class="fa fa-check"></i><b>4.5.10</b> Polinômio característico</a></li>
<li class="chapter" data-level="4.5.11" data-path="st.html"><a href="st.html#estacionariedade-e-causalidade-de-um-processo-arma"><i class="fa fa-check"></i><b>4.5.11</b> Estacionariedade e causalidade de um processo ARMA</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="rev.html"><a href="rev.html#exercícios"><i class="fa fa-check"></i><b>4.6</b> Exercícios</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="stnoest.html"><a href="stnoest.html"><i class="fa fa-check"></i><b>5</b> Séries temporais não estacionárias</a><ul>
<li class="chapter" data-level="5.1" data-path="stnoest.html"><a href="stnoest.html#como-lidar-com-tendências-determinísticas"><i class="fa fa-check"></i><b>5.1</b> Como lidar com tendências determinísticas</a></li>
<li class="chapter" data-level="5.2" data-path="stnoest.html"><a href="stnoest.html#testes-de-raiz-unitária"><i class="fa fa-check"></i><b>5.2</b> Testes de raiz unitária</a><ul>
<li class="chapter" data-level="5.2.1" data-path="stnoest.html"><a href="stnoest.html#identificando-tendência-estocástica"><i class="fa fa-check"></i><b>5.2.1</b> Identificando tendência estocástica</a></li>
<li class="chapter" data-level="5.2.2" data-path="stnoest.html"><a href="stnoest.html#teste-de-dickey-fuller-df"><i class="fa fa-check"></i><b>5.2.2</b> Teste de Dickey Fuller (<em>DF</em>)</a></li>
<li class="chapter" data-level="5.2.3" data-path="stnoest.html"><a href="stnoest.html#teste-adf"><i class="fa fa-check"></i><b>5.2.3</b> Teste ADF</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="stnoest.html"><a href="stnoest.html#eliminando-tendência-estocástica"><i class="fa fa-check"></i><b>5.3</b> Eliminando tendência estocástica</a><ul>
<li class="chapter" data-level="5.3.1" data-path="stnoest.html"><a href="stnoest.html#diferenças-sucessivas"><i class="fa fa-check"></i><b>5.3.1</b> Diferenças sucessivas</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="stnoest.html"><a href="stnoest.html#modelagem-arima"><i class="fa fa-check"></i><b>5.4</b> Modelagem ARIMA</a></li>
<li class="chapter" data-level="5.5" data-path="stnoest.html"><a href="stnoest.html#previsão"><i class="fa fa-check"></i><b>5.5</b> Previsão</a><ul>
<li class="chapter" data-level="5.5.1" data-path="stnoest.html"><a href="stnoest.html#erro-de-previsão"><i class="fa fa-check"></i><b>5.5.1</b> Erro de previsão</a></li>
<li class="chapter" data-level="5.5.2" data-path="stnoest.html"><a href="stnoest.html#medidas-de-desempenho"><i class="fa fa-check"></i><b>5.5.2</b> Medidas de desempenho</a></li>
<li class="chapter" data-level="5.5.3" data-path="stnoest.html"><a href="stnoest.html#previsão-dinâmica-e-estática"><i class="fa fa-check"></i><b>5.5.3</b> Previsão dinâmica e estática</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="stnoest.html"><a href="stnoest.html#regressão-espúria-e-cointegração"><i class="fa fa-check"></i><b>5.6</b> Regressão espúria e cointegração</a><ul>
<li class="chapter" data-level="5.6.1" data-path="stnoest.html"><a href="stnoest.html#quando-é-possível-regredir-duas-séries-id"><i class="fa fa-check"></i><b>5.6.1</b> Quando é possível regredir duas séries I(d)</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="rev.html"><a href="rev.html#exercícios"><i class="fa fa-check"></i><b>5.7</b> Exercícios</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="vi.html"><a href="vi.html"><i class="fa fa-check"></i><b>6</b> Variáveis instrumentais</a><ul>
<li class="chapter" data-level="6.1" data-path="vi.html"><a href="vi.html#conceito-da-exogeneidade-dos-regressores"><i class="fa fa-check"></i><b>6.1</b> Conceito da exogeneidade dos regressores</a><ul>
<li class="chapter" data-level="6.1.1" data-path="vi.html"><a href="vi.html#variável-proxy"><i class="fa fa-check"></i><b>6.1.1</b> Variável proxy</a></li>
<li class="chapter" data-level="6.1.2" data-path="vi.html"><a href="vi.html#variável-omitida"><i class="fa fa-check"></i><b>6.1.2</b> Variável omitida</a></li>
<li class="chapter" data-level="6.1.3" data-path="vi.html"><a href="vi.html#erros-de-mensuração"><i class="fa fa-check"></i><b>6.1.3</b> Erros de mensuração</a></li>
<li class="chapter" data-level="6.1.4" data-path="vi.html"><a href="vi.html#variável-com-erro-de-medição"><i class="fa fa-check"></i><b>6.1.4</b> Variável com erro de medição</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="vi.html"><a href="vi.html#variável-instrumental"><i class="fa fa-check"></i><b>6.2</b> Variável instrumental</a><ul>
<li class="chapter" data-level="6.2.1" data-path="vi.html"><a href="vi.html#diferença-entre-proxy-e-variável-instrumental"><i class="fa fa-check"></i><b>6.2.1</b> Diferença entre proxy e variável instrumental</a></li>
<li class="chapter" data-level="6.2.2" data-path="vi.html"><a href="vi.html#o-método-de-estimação-por-variável-instrumental"><i class="fa fa-check"></i><b>6.2.2</b> O método de estimação por variável instrumental</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="vi.html"><a href="vi.html#inferência-com-o-estimador-por-variável-instrumental"><i class="fa fa-check"></i><b>6.3</b> Inferência com o estimador por variável instrumental</a><ul>
<li class="chapter" data-level="6.3.1" data-path="vi.html"><a href="vi.html#teste-de-hipóteses"><i class="fa fa-check"></i><b>6.3.1</b> Teste de hipóteses</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="vi.html"><a href="vi.html#variáveis-instrumentais-em-modelos-de-regressão-múltipla"><i class="fa fa-check"></i><b>6.4</b> Variáveis instrumentais em modelos de regressão múltipla</a><ul>
<li class="chapter" data-level="6.4.1" data-path="vi.html"><a href="vi.html#mínimos-quadrados-em-dois-estágios-mq2e"><i class="fa fa-check"></i><b>6.4.1</b> Mínimos quadrados em dois estágios (MQ2E)</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="vi.html"><a href="vi.html#testes-de-endogeneidade"><i class="fa fa-check"></i><b>6.5</b> Testes de endogeneidade</a><ul>
<li class="chapter" data-level="6.5.1" data-path="vi.html"><a href="vi.html#teste-de-hausmann"><i class="fa fa-check"></i><b>6.5.1</b> Teste de Hausmann</a></li>
<li class="chapter" data-level="6.5.2" data-path="vi.html"><a href="vi.html#teste-de-regressão"><i class="fa fa-check"></i><b>6.5.2</b> Teste de regressão</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="rev.html"><a href="rev.html#exercícios"><i class="fa fa-check"></i><b>6.6</b> Exercícios</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Econometria e Séries Temporais com R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="rev" class="section level1">
<h1><span class="header-section-number">Capítulo 2</span> Revisão</h1>
<p>Esse capítulo é uma breve revisão sobre conceitos estatísticos que são fundamentais para entender os conceitos da modelagem econométrica.</p>
<div id="aleatoriedade-a-essência-da-estatística" class="section level2">
<h2><span class="header-section-number">2.1</span> Aleatoriedade, a essência da estatística</h2>
<p>Para iniciar qualquer curso em que são utilizadas técnicas estatísticas, é necessário
esclarecer/fundamentar bem o conceito de aleatoriedade.</p>
<blockquote>
<p><em>Na história antiga, os conceitos de chance e de aleatoriedade eram interligados ao conceito que era atribuído a destino. Várias pessoas da antiguidade jogavam dados para determinarem o destino, e posteriormente isso se desenvolveu em jogos de azar. A maioria das culturas usaram vários métodos de adivinhações para tentarem contornar a aleatoriedade e o destino, ou mesmo a dita sorte. A palavra aleatoriedade é utilizada para exprimir quebra de ordem, propósito, causa, ou imprevisibilidade em uma terminologia não científica. Um processo aleatório é o processo repetitivo cujo resultado não descreve um padrão determinístico, mas segue uma distribuição de probabilidade <a href="https://pt.wikipedia.org/wiki/Aleatoriedade">Wikipédia</a>.</em></p>
</blockquote>
<div class="figure" style="text-align: center"><span id="fig:dados"></span>
<img src="Figuras/dados-vp.jpg" alt="Dados" width="33%" />
<p class="caption">
Figura 2.1: Dados
</p>
</div>
<p>As técnicas estatísticas surgem para encontrar algum <em>padrão de variação</em>. Para tal tarefa é necessário formalizar e definir alguns conceitos, como são os casos de variável aleatória e distribuição de probabilidade.</p>
</div>
<div id="variável-aleatória" class="section level2">
<h2><span class="header-section-number">2.2</span> Variável aleatória</h2>
<p>Denomina-se <strong>variável</strong> uma propriedade (característica) qualquer das unidades da população para a qual foi definida uma unidade de medida, que pode ser quantitativa ou qualitativa. Observe que essa característica é comum a todos os indivíduos e portanto é uma característica da população. Em geral, queremos fazer afirmações sobre características e temos apenas informações de alguns indivíduos (amostra). Assim, toda afirmação feita a partir de uma amostra é passível de erros, ou seja, é uma
aproximação. Além disso, em alguns casos não é possível “medir” toda a população e devemos pensar nessa característica como uma quantidade aleatória. Para isso, é necessário introduzirmos o
conceito de <em><strong>variável aleatória</strong></em>.</p>

<div class="definition">
<span id="def:EspacoAmostral" class="definition"><strong>Definição 2.1  (Espaço Amostral)  </strong></span>Espaço amostral de um <em>experimento aleatório</em> (fenômeno que, mesmo repetidos várias vezes sob condições semelhantes, apresentam resultados imprevisíveis) é <strong>qualquer</strong> conjunto contendo todos os possíveis resultados do experimento. Aqui, sempre que não houver perigo de confusão, o espaço amostral de um experimento em questão será denotado por <span class="math inline">\(\Omega\)</span>.
</div>

<p><br></p>

<div class="example">
<span id="exm:exampcel" class="example"><strong>Exemplo 2.1  </strong></span>No seguinte experimento: derrubar o celular e observar a face voltada para cima, o espaço amostral é o conjunto <span class="math inline">\(\{\mathrm{Frontal}, \mathrm{Costas}\}\)</span>.
</div>

<p><br></p>

<div class="example">
<span id="exm:edados" class="example"><strong>Exemplo 2.2  </strong></span>Se o experimento é lançar um dado de seis faces, o espaço amostral é <span class="math inline">\(\{1,2,3,4,5,6\}\)</span>.
</div>

<p><br></p>

<div class="example">
<span id="exm:exampespamost" class="example"><strong>Exemplo 2.3  </strong></span>Poderá perfeitamente existir mais de um espaço amostral adequado para um determinado experimento. No Exemplo <a href="rev.html#exm:edados">2.2</a>, o conjunto <span class="math inline">\(\{1,2,3,4,5,6,7\}\)</span> contém todos os possíveis resultados do experimento em questão (lançar um dado de seis faces). Assim, pela definição <a href="rev.html#def:EspacoAmostral">2.1</a>, este conjunto é tão adequado como espaço amostral quanto o conjunto mais intuitivo <span class="math inline">\(\{1,2,3,4,5,6\}\)</span>. Até mesmo o conjunto dos números reais <span class="math inline">\(\mathbb{R}\)</span> é adequado. Obviamente, sempre que possível é recomendável utilizar o conjunto mais <em>natural</em> como espaço amostral, porém, do ponto de vista teórico, desde que o conjunto escolhido efetivamente contenha todos os possíveis resultados do experimento, não faz diferença alguma qual conjunto se está utilizando.
</div>

<p><br></p>

<div class="example">
<span id="exm:examexp" class="example"><strong>Exemplo 2.4  </strong></span>Nos exemplos anteriores, é possível (e muito fácil) determinar exatamente quais são todos os possíveis resultados dos experimentos em questão. Porém nem sempre este é o caso. Considere o experimento em que uma pessoa é escolhida ao acaso e sua altura (em metros) medida. Neste caso é difícil determinar precisamente o conjunto contendo exatamente todos os possíveis resultados do experimento. Com certeza o conjunto <span class="math inline">\([0,10]\)</span> contém todas as possíveis alturas a serem registradas. O conjunto <span class="math inline">\([0,3]\)</span> também. Por outro lado, será que o conjunto <span class="math inline">\([0,2.7]\)</span> é apropriado? E <span class="math inline">\((0.3,2.7)\)</span>?
</div>

<p>Todo subconjunto de um espaço amostral é chamado <em>evento</em>. Os
subconjuntos de um espaço amostral contendo apenas um elemento são
chamados de <em>eventos elementares</em>.</p>
<p>Por exemplo, no lançamento de um dado de seis faces, <span class="math inline">\(\{5\}\)</span> é um evento elementar. Outro evento possível é: <em>a face superior é ímpar</em>, o que é equivalente ao subconjunto <span class="math inline">\(\{1,3,5\}\subset\Omega\)</span>. Outra possibilidade poderia ser verificar se a face obtida é superior a 3.</p>
<p>Existem ainda experimentos que podem ser vistos como “compostos” por natureza, como por exemplo o lançamento independente de um dado de seis faces e de uma moeda honesta, no qual anotamos a face superior do dado e a face da moeda. Neste caso, é fácil determinar um espaço amostral associado ao experimento que contenha exatamente todos os resultados possíveis. Este constituirá de pares contendo um número inteiro de 0 à 6, correspondente ao lançamento do dado e um elemento do conjunto <span class="math inline">\(\{\mathrm{Frontal},\mathrm{Costas}\}\)</span>, correspondente à queda do celular, ou seja, <span class="math inline">\(\Omega=\{(1, \mathrm{Frontal}), (1,\mathrm{costas}), \cdots, (6, \mathrm{Frontal}), (6,\mathrm{Costas})\}\)</span>. Uma outra maneira de representar isto é a partir do produto cartesiano dos espaços amostrais de cada um dos experimentos individuais, neste caso <span class="math inline">\(\Omega=\{1,2,3,4,5,6\}\times\{\mathrm{Frontal},\mathrm{Costas}\}\)</span>.</p>
<p>Espaços amostrais são importantes na definição de um <em>espaço de probabilidade</em>. Um espaço de probabilidade <span class="math inline">\((\Omega, \mathcal{F},\mathcal{P})\)</span> onde <span class="math inline">\(\Omega\)</span> denota um espaço amostral qualquer, <span class="math inline">\(\mathcal{F}\)</span> é um conjunto de eventos associado à <span class="math inline">\(\Omega\)</span> satisfazendo certas propriedades (<span class="math inline">\(\sigma\)</span>-algebra de eventos), e <span class="math inline">\(\mathcal{P}:\mathcal{F}\rightarrow[0,1]\)</span> uma medida de probabilidade atribuindo valores em <span class="math inline">\([0,1]\)</span> para cada evento de interesse em <span class="math inline">\(\mathcal{F}\)</span> (a probabilidade dos eventos).</p>
<blockquote>
<p><em>Uma <strong>variável aleatória</strong> é uma função do espaço amostral
<span class="math inline">\(\Omega\)</span> nos reais, para a qual é possível calcular a
probabilidade de ocorrência de seus valores. Em geral, as
variáveis aleatórias são representadas por letras maiúsculas do
fim do alfabeto. Temos, para cada elemento <span class="math inline">\(\omega \in \Omega\)</span>, um
número real <span class="math inline">\(X(\omega)\)</span> conforme a Figura <a href="rev.html#fig:dados">2.1</a>.</em></p>
</blockquote>
<div class="figure" style="text-align: center"><span id="fig:va"></span>
<img src="Figuras/VA.jpg" alt="Variável aleatória" width="33%" />
<p class="caption">
Figura 2.2: Variável aleatória
</p>
</div>
<p>Garantimos o cálculo de probabilidades com variáveis aleatórias ao exigir que, para qualquer <span class="math inline">\(I \subset \mathbb{R}\)</span>, o conjunto <span class="math inline">\(X^{-1}(I)\)</span> seja um evento. Em outras palavras, o conjunto <span class="math inline">\(X^{-1}(I)\)</span> é um
elemento de <span class="math inline">\(\mathcal{F}\)</span>, ou seja, <span class="math inline">\(X^{-1}(I) \in \mathcal{F}\)</span>. Lembremos que apenas os elementos de
<span class="math inline">\(\mathcal{F}\)</span> têm atribuição de probabilidade. Em linguagem mais matemática, dizemos que uma variável aleatória é qualquer função mensurável em <span class="math inline">\((\Omega,\mathcal{F})\)</span>. Isto justifica dizer que a
variável <span class="math inline">\(X\)</span> é <span class="math inline">\(\mathcal{F}\)</span>-. Com frequência, faz-se menção ao espaço de probabilidade <span class="math inline">\((\Omega, \mathcal{F},\mathcal{P})\)</span>, para deixar claro o espaço amostral, a <span class="math inline">\(\sigma\)</span>-álgebra e a probabilidade envolvidas. Formalmente, definimos</p>
<p><br></p>

<div class="definition">
<span id="def:Def1" class="definition"><strong>Definição 2.2  </strong></span>Seja <span class="math inline">\((\Omega, \mathcal{F}, \mathcal{P})\)</span> um espaço de
probabilidade. Denominamos de variável aleatória, qualquer função
<span class="math inline">\(X:\Omega \rightarrow \mathbb{R}\)</span> tal que
<span class="math display">\[\begin{equation*}
    X^{-1}(I)=\{\omega \in \Omega : X(\omega) \in I\} \in
    \mathcal{F},
\end{equation*}\]</span>
para todo intervalo <span class="math inline">\(I \subset \mathbb{R}\)</span>. Em palavras, <span class="math inline">\(X\)</span> é tal que sua
imagem inversa de intervalos <span class="math inline">\(I \subset \mathbb{R}\)</span> pertencem a
<span class="math inline">\(\sigma\)</span>-álgebra <span class="math inline">\(\mathcal{F}\)</span>.
</div>

<p>No que segue precisamos do conceito de cardinalidade de um conjunto. Em palavras simples, a cardinalidade de um conjunto é uma maneira de expressar a “quantidade” de elementos que este contém. Um conjunto ordenado <span class="math inline">\(A\)</span> é dito <em>finito</em> se contém um número finito de elementos. A cardinalidade de um conjunto finito nada mais é que o número de elementos que este contém. Por exemplo o conjunto <span class="math inline">\(A=\{1,2,9,15\}\)</span> é finito e tem cardinalidade 4.</p>
<p>Por outro lado, a definição de cardinalidade para conjuntos infinitos é matematicamente muito mais complexa pois, no final das contas, a ideia é impor uma hierarquia, uma “ordem”, no “tamanho” de conjuntos infinitos. Obviamente a cardinalidade de um conjunto infinito não pode ser expressa em números. Estamos interessados apenas em distinguir entre dois “tamanhos” de conjuntos infinitos: enumerável e não-enumerável. Por sorte, na maioria das vezes é possível utilizar apenas a intuição para resolver o problema. Intuitivamente, um conjunto ordenado <span class="math inline">\(A\)</span> é dito ser infinito
enumerável (ou ainda, <em>contável</em>) se dado um elemento qualquer de <span class="math inline">\(A\)</span>, podemos determinar quem é o próximo elemento do conjunto. Caso contrário, o conjunto é dito ser <em>não-enumerável</em>. Por exemplo, o conjunto dos números naturais <span class="math inline">\(\mathbb{N}\)</span> é infinito enumerável. De fato, dado qualquer número natural <span class="math inline">\(x\)</span>, o próximo é <span class="math inline">\(x+1\)</span>, obviamente. Já o conjunto <span class="math inline">\([0,1]\)</span> é infinito não-enumerável. Por exemplo, dado o número <span class="math inline">\(0.5\in[0,1]\)</span>, qual é próximo elemento de <span class="math inline">\([0,1]\)</span>? Poderíamos dizer 0.6, mas e 0.51? Este ainda está mais longe de 0.5 que <span class="math inline">\(0.501\)</span>. De fato <span class="math inline">\(0,5001\)</span>, <span class="math inline">\(0.50001\)</span> etc. é uma sequência infinita de números em <span class="math inline">\([0,1]\)</span> cada vez mais próxima de 0.5 de forma que não é possível determinar o próximo elemento na ordenação do conjunto.
Os conjuntos enumeráveis mais conhecidos são <span class="math inline">\(\mathbb{N}\)</span>, <span class="math inline">\(\mathbb{Z}\)</span> e <span class="math inline">\(\mathbb{Q}\)</span>, sendo que este último é um pouco mais difícil de aplicar a regra intuitiva acima. Os conjuntos não enumeráveis mais conhecidos são <span class="math inline">\(\mathbb{R}\)</span>, <span class="math inline">\(\mathbb{R}\setminus\mathbb{Q}\)</span>, <span class="math inline">\(\mathbb{C}\)</span>.</p>
<p><br></p>

<div class="definition">
<span id="def:vad" class="definition"><strong>Definição 2.3  (Variável Aleatória Discreta)  </strong></span>Se o conjunto dos possíveis valores da variável aleatória é finito ou infinito enumerável.
</div>

<p><br></p>

<div class="definition">
<span id="def:vac" class="definition"><strong>Definição 2.4  (Variável Aleatória Contínua)  </strong></span>Se o conjunto dos possíveis valores da variável aleatória é infinito não-enumerável.
</div>

<p><br></p>
<p>Na prática, é comum a utilização de variáveis aleatórias contínuas pois estas são matematicamente mais simples de se tratar. Quando, por exemplo, falamos que a renda é uma v.a. contínua (na verdade ela é
discreta) é pela conveniência da aproximação.</p>
<div id="distribuição-de-probabilidade" class="section level3">
<h3><span class="header-section-number">2.2.1</span> Distribuição de probabilidade</h3>
<p>A função que descreve as probabilidades da variável aleatória discreta <span class="math inline">\(X\)</span> assumir os diferentes valores do espaço amostral é chamada de função massa de probabilidade. No caso de uma variável contínua, a probabilidade de uma variável aleatória assumir qualquer valor específico é 0. Neste caso o análogo da função massa de probabilidade é a função de densidade de probabilidade (abreviado f.d.p. ou ainda, do inglês, p.d.f.) que, em poucas palavras, descreve a variação instantânea da probabilidade no ponto. Para que uma função qualquer <span class="math inline">\(f\)</span> seja uma densidade de probabilidade é necessário que</p>
<p><span class="math display" id="eq:fdensidade">\[\begin{align}
 &amp;f(x)\geq0\quad \mbox{ para todo } x\in\mathbb{R},\nonumber\\
&amp;\int_{\mathbb{R}}f(x)dx=\int_{-\infty}^{\infty}f(x)dx=1.
\tag{2.1}
\end{align}\]</span></p>
<p>Como a probabilidade de ocorrência de um valor em particular de uma variávela aleatória contínua é sempre 0, probabilidades são discutidas em termos de intervalos, ou mesmo outros tipos de conjuntos. Essas probabilidades são obtidas por meio de
integração da função densidade no intervalo especificado. Por exemplo, seja <span class="math inline">\(X\)</span> uma variávela aleatória com densidade <span class="math inline">\(f(x)\)</span>. Então <span class="math inline">\(P(a \leq X \leq b)\)</span> é dada por
<span class="math display">\[P(a \leq X \leq b)=\int_a^b f(x)dx.\]</span>
Analogamente, para um conjunto <span class="math inline">\(A\subseteq \mathbb{R}\)</span> qualquer,
<span class="math display">\[P(X\in A)=\int_A f(x)dx.\]</span></p>
<p>A probabilidade de que a variável aleatória <span class="math inline">\(X\)</span> assuma valores inferiores ou igual a um número <span class="math inline">\(x\in\mathbb{R}\)</span>, <span class="math inline">\(P(X\leq x)\)</span>, possui importancia intrínsica pois representa a probabilidade acumulada até o ponto <span class="math inline">\(x\)</span>. Por isso, para cada <span class="math inline">\(x\in\mathbb{R}\)</span> fixo, denotamos esta probabilidade por
<span class="math display">\[F(x)=P(X\leq x)\]</span>
e a função assim definida <span class="math inline">\(F:\mathbb{R}\rightarrow[0,1]\)</span> é chamada
de função de distribuição acumulada (denotada por f.d.a.), ou
somente função de distribuição. Note que se <span class="math inline">\(X\)</span> é uma variável aleatória contínua com densidade <span class="math inline">\(f\)</span>,
<span class="math display">\[F(x)=P(X \leq x)=\int_{-\infty}^x f(t)dt.\]</span></p>
</div>
<div id="distribuições-conjunta-marginal-e-condicional" class="section level3">
<h3><span class="header-section-number">2.2.2</span> Distribuições conjunta, marginal e condicional</h3>
<p>Geralmente estamos interessados não apenas numa variável aleatória mas na relação entre algumas variáveis aleatórias. Suponha que temos duas variáveis aleatórias, <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span>. Agora além do comportamento probabilístico individual de <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span>, caracterizado por suas funções de distribuições, digamos <span class="math inline">\(F_X\)</span> e <span class="math inline">\(F_Y\)</span>, respectivamente, precisamos alguma forma de descrever o comportamento probabilístico conjunto de <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span>. Para isso definimos a função de distribuição acumulada de <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span>, denotada por <span class="math inline">\(F_{X,Y}\)</span>, por <span class="math display">\[F_{X,Y}(x,y)=P(X\leq x, Y\leq y).\]</span></p>
<p>Se <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span> são ambas contínuas, podemos definir a densidade conjunta de <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span> denotada por <span class="math inline">\(f_{X,Y}\)</span>, como sendo a função que satisfaz
<span class="math display">\[F_{X,Y}(x,y)=\int_{-\infty}^x \int_{-\infty}^y f_{X,Y}(z,w)dzdw.\]</span></p>
<p>A função de distribuição conjunta de um par de variáveis aleatórias <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span> caracteriza também os comportamentos probabilisticos de <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span> individualmente. De fato
<span class="math display">\[F_X(x)=\lim_{y\rightarrow\infty}F_{X,Y}(x,y) \quad \mbox{ e }\quad F_Y(y)=\lim_{x\rightarrow\infty}F_{X,Y}(x,y)\]</span>
e também
<span class="math display">\[\begin{equation*}
f_X(x)=\int_{\mathbb{R}}f_{X,Y}(x,y)dy\quad\mbox{e}\quad f_Y(y)=\int_{\mathbb{R}}f_{X,Y}(x,y)dx.
\end{equation*}\]</span>
Quando temos a função de distribuição conjunta de um par <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span> de variáveis aleatórias, dizemos que as densidades/distribuições individuais de <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span> são as densidades/distribuições marginais de <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span>.</p>
<p>A função de distribuição condicional de <span class="math inline">\(X\)</span> dado <span class="math inline">\(Y=y\)</span> é descrita por
<span class="math display">\[F_{X|Y}(x|y)=P(X\leq x|Y=y)
=\left\{\begin{array}{cc} \frac{P(X\leq x,Y=y)}{P(Y=y)}\,, &amp; \mbox{ se $X$ é discreta e }P(Y=y)\neq 0 \,\,\, \\
\frac{\int_{-\infty}^x f_{X,Y}(t,y)dt}{f_y(y)}\,, &amp; \mbox{ se $X$ é contínua e } f_Y(y)\neq 0\end{array}\right.
\]</span></p>
<ul>
<li>As densidades condicionais são:</li>
</ul>
<div class="line-block">   a) <span class="math inline">\(f_{X|Y}(x|y)\)</span>, que é a densidade de <span class="math inline">\(X\)</span> dado <span class="math inline">\(Y=y\)</span>;</div>
<div class="line-block">   b) <span class="math inline">\(f_{Y|X}(y|x)\)</span>, que é a densidade de <span class="math inline">\(Y\)</span> dado <span class="math inline">\(X=x\)</span>.</div>
<p>Formalmente, temos a relação
<span class="math display">\[F_{X|Y}(x|y)=\int_{-\infty}^xf_{X|Y}(t|y)dt\quad \mbox{e} \quad F_{Y|x}(y|x)=\int_{-\infty}^yf_{Y|X}(t|x)dt,\]</span>
no caso em que <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span> são contínuas. Relações parecidas valem no caso em que <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span> são discretas, trocando-se integrais por somas e densidades por função massa de probabilidade.</p>
<p>A densidade conjunta pode ser escrita como o produto das
densidades marginal e condicional da seguinte forma:
<span class="math display">\[\begin{eqnarray*}
f_{X,Y}(x,y)&amp;=&amp;f_X(x)f_{Y|X}(y|x)\\
      &amp;=&amp;f_Y(y)f_{X|Y}(x|y).
\end{eqnarray*}\]</span>
Se <span class="math inline">\(f_{X,Y}(x,y)=f_X(x)f_Y(y)\)</span> para todo <span class="math inline">\(x\)</span> e <span class="math inline">\(y\)</span>, então <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span> são
chamadas de variáveis . Note que, se eles são
independentes,
<span class="math display">\[f_{X|Y}(x|y)=f_X(x) \quad \mbox{e} \quad f_{Y|X}(y|x)=f_Y(y),\]</span>
isto é, as distribuições condicionais são as mesmas que as marginais. Intuitivamente, quando <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span> são independentes <span class="math inline">\(X\)</span> não carrega nenhuma informação útil a respeito de <span class="math inline">\(Y\)</span>, assim o fato de <span class="math inline">\(Y\)</span> ser ou não conhecido é irrelevante para a determinação de <span class="math inline">\(X\)</span>.</p>
</div>
</div>
<div id="a-distribuição-normal-e-distribuições-relacionadas" class="section level2">
<h2><span class="header-section-number">2.3</span> A distribuição Normal e distribuições relacionadas</h2>
<p>Existem algumas distribuições de probabilidade cujas probabilidades que, devido à sua utilização em diversas aplicações, valores de suas funções de distribuição são tabuladas. Dentre estas distribuições notáveis, podemos citar distribuição normal e as distribuições <span class="math inline">\(\chi^2\)</span>, <span class="math inline">\(t\)</span> e <span class="math inline">\(F\)</span>, as quais discutiremos juntamente com as distribuições lognormal e normal bivariada. Existem diversas outras distribuições para as quais tabelas extensivas estão disponíveis. Como exemplos citamos as distribuições gama e beta. Na verdade, a distribuição <span class="math inline">\(\chi^2\)</span> é um caso particular da distribuição gama, e as distribuições <span class="math inline">\(t\)</span> e <span class="math inline">\(F\)</span> são casos particulares da distribuição beta. Trataremos aqui apenas das citadas.</p>
<p>Existe um grande criticismo sobre a adequação da distribuição normal para descrever variáveis econômicas. Muitas vezes a distribuição normal de fato não é apropriada. Contudo, dois fatos tornam o estudo da distribuição normal importantes: primeiramente, embora existam problemas em que o uso da distribuição normal é questionável, existe um número muito maior de problemas em que o uso desta é totalmente apropriado. Segundo, mesmo que as variáveis não sejam normalmente distribuídas, pode-se considerar transformações de variáveis que façam com que as variáveis transformadas se tornem normalmente distribuídas.</p>
<div id="a-distribuição-normal" class="section level3">
<h3><span class="header-section-number">2.3.1</span> A distribuição Normal</h3>
<p>A distribuição normal, cuja densidade possui um formato que lembra um sino, é a distribuição
mais amplamente utilizada em aplicações estatísticas numa grande variedade de áreas. Dizemos que <span class="math inline">\(X\)</span> tem distribuição normal com média <span class="math inline">\(\mu\in\mathbb{R}\)</span> e variância <span class="math inline">\(\sigma^2&gt;0\)</span>, denotado compactamente por <span class="math inline">\(X\sim N(\mu,\sigma^2)\)</span>, se sua função de densidade de probabilidade for dada por
<span class="math display">\[f(x)=\frac{1}{\sigma \sqrt{2\pi}}\exp\left[-\frac{1}{2\sigma^2}(x-\mu)^2\right], \quad \mbox{para } x\in\mathbb{R}.\]</span>
Os parâmetros <span class="math inline">\(\mu\)</span> e <span class="math inline">\(\sigma^2\)</span> são também
chamados de parâmetros de locação e escala, respectivamente.</p>
<div class="figure" style="text-align: center"><span id="fig:LEN"></span>
<img src="Figuras/Loc_Esc_Normal.jpg" alt="Função densidade Normal com diferentes parâmetros de locação  e escala." width="80%" />
<p class="caption">
Figura 2.3: Função densidade Normal com diferentes parâmetros de locação e escala.
</p>
</div>
<p>Se <span class="math inline">\(\mu = 0\)</span> e <span class="math inline">\(\sigma = 1\)</span>, a distribuição é chamada de “distribuição normal padrão”
e a função de densidade de probabilidade reduz-se a,
<span class="math display">\[f(x) = \frac{1}{\sqrt{2\pi}} \, e^{-\frac{x^2}{2}}.\]</span></p>
<p>Uma propriedade importante propriedade da distribuição normal é que qualquer
combinação linear de variáveis normalmente distribuídas também é
normalmente distribuída. De fato, pode-se mostrar que, se
<span class="math display">\[X_1 \sim N(\mu_1,\sigma^2_1)
\quad \mbox{e} \quad X_2 \sim N(\mu_2,\sigma^2_2)\]</span>
e a correlação entre <span class="math inline">\(X_1\)</span> e <span class="math inline">\(X_2\)</span> é <span class="math inline">\(\rho\)</span>, então
<span class="math display">\[a_1X_1+a_2X_2 \sim N(a_1\mu_1+a_2\mu_2, a_1^2\sigma^2_1+a_2^2\sigma^2_2+2\rho
a_1a_2\sigma_1\sigma_2).\]</span>
Em particular,</p>
<p><span class="math display">\[X_1+X_2 \sim N(\mu_1+\mu_2, \sigma^2_1+\sigma^2_2+2\rho \sigma_1\sigma_2)\]</span>
e
<span class="math display">\[X_1-X_2 \sim N(\mu_1-\mu_2, \sigma^2_1+\sigma^2_2-2\rho \sigma_1\sigma_2).\]</span></p>
</div>
<div id="distribuições-relacionadas" class="section level3">
<h3><span class="header-section-number">2.3.2</span> Distribuições relacionadas</h3>
<p>Além da distribuição normal, há outras distribuições de probabilidade que usaremos com frequência. São elas as distribuições <span class="math inline">\(\chi^2\)</span>, <span class="math inline">\(t\)</span> e <span class="math inline">\(F\)</span>, tabuladas no apêndice. Estas distribuições são derivadas da distribuição normal e definidas como descrito a seguir.</p>
<div id="distribuição-chi2" class="section level4">
<h4><span class="header-section-number">2.3.2.1</span> Distribuição <span class="math inline">\(\chi^2\)</span></h4>
<p>A distribuição <span class="math inline">\(\chi^2\)</span> é bastante importante em aplicações e é definida a partir da soma dos quadrados de variáveis normais. Mais especificamente, se <span class="math inline">\(X_1, X_2, \cdots, X_n\)</span> são variáveis aleatórias independentes com distribuição normal padrão então
<span class="math display">\[Q=\sum_{i=1}^n X_i^2\]</span>
tem distribuição <span class="math inline">\(\chi^2\)</span> com <span class="math inline">\(n\)</span> graus de liberdade (g.l.), e escrevemos isso compactamente como <span class="math inline">\(Q \sim \chi_n^2\)</span>.</p>
<p>Se <span class="math inline">\(X_i \sim N(\mu, \sigma^2)\)</span>, então <span class="math inline">\(Q\)</span> deve ser definido por</p>
<p><span class="math display">\[Q=\sum_{i=1}^n \frac{(X_i-\mu)^2}{\sigma^2}.\]</span>
A distribuição <span class="math inline">\(\chi^2\)</span> também satisfaz uma determinada “propriedade de adição”,
no seguinte sentido: se <span class="math inline">\(Z_1 \sim \chi_n^2\)</span> e <span class="math inline">\(Z_2 \sim \chi_m^2\)</span> e <span class="math inline">\(Z_1\)</span> e <span class="math inline">\(Z_2\)</span> são
independentes, então <span class="math inline">\(Z_1+Z_2 \sim \chi^2_{n+m}\)</span>. Note que esta propriedade de adição é bem mais restritiva que aquela da distribuição normal, já que exige independência para que a simples soma das variáveis satisfaçam à propriedade (para normal, a propriedade vale para combinações lineares quaisquer), mas ainda assim é muito útil na prática.</p>
</div>
<div id="distribuição-t" class="section level4">
<h4><span class="header-section-number">2.3.2.2</span> Distribuição <span class="math inline">\(t\)</span></h4>
<p>Se <span class="math inline">\(X \sim N(0,1)\)</span>, <span class="math inline">\(Y \sim \chi^2_n\)</span>, e <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span> são independentes, a variável
<span class="math display">\[T=\frac{X}{\sqrt{Y/n}}=\frac{\sqrt{n}X}{\sqrt{Y}}\]</span>
possui distribuição <span class="math inline">\(t\)</span> com <span class="math inline">\(n\)</span> g.l. Escrevemos isso como <span class="math inline">\(T \sim t_n\)</span>. O subscrito <span class="math inline">\(n\)</span> novamente
denota os g.l. Assim como a distribuição normal, a distribuição <span class="math inline">\(t\)</span> é uma distribuição de probabilidade simétrica, com forma lembrando um sino, sendo porém mais achatada e com caudas mais “pesadas” que a normal. Quando o número de graus de liberdade <span class="math inline">\(n\)</span> de uma variável <span class="math inline">\(t_n\)</span> tende ao infinito, obtemos a distribuição normal. Em outras palavras, quando os graus de liberdade de uma variável aleatória com distribuição <span class="math inline">\(t_n\)</span> for grande, esta tem comportamento probabilístico muito similar ao de uma normal.</p>
</div>
<div id="distribuição-f" class="section level4">
<h4><span class="header-section-number">2.3.2.3</span> Distribuição <span class="math inline">\(F\)</span></h4>
<p>Se <span class="math inline">\(Y_1 \sim \chi^2_{n1}\)</span>, <span class="math inline">\(Y_2 \sim \chi^2_{n2}\)</span> e <span class="math inline">\(Y_1\)</span> e <span class="math inline">\(Y_2\)</span> são independentes, a variável
<span class="math display">\[F=\frac{Y_1/n_1}{Y_2/n_2}=\frac{n_2Y_1}{n_1Y_2}\]</span>
é dita possuir distribuição <span class="math inline">\(F\)</span> com <span class="math inline">\(n_1\)</span> e <span class="math inline">\(n_2\)</span> g.l. Escrevemos isso como <span class="math inline">\(F\sim F_{n_1,n_2}\)</span>. O primeiro subscrito <span class="math inline">\(n_1\)</span>, refere-se aos g.l. do numerador, e o segundo subscrito, <span class="math inline">\(n_2\)</span>, refere-se aos g.l. do denominador.</p>
<div class="figure" style="text-align: center"><span id="fig:len"></span>
<img src="Figuras/chi_t_F_density.jpg" alt="Função densidade Qui-Quadrado, t-Student e F-Snedecor. Em parênteses os graus de liberdade." width="80%" />
<p class="caption">
Figura 2.4: Função densidade Qui-Quadrado, t-Student e F-Snedecor. Em parênteses os graus de liberdade.
</p>
</div>
</div>
</div>
</div>
<div id="parâmetros-estimadores-e-valores-estimados" class="section level2">
<h2><span class="header-section-number">2.4</span> Parâmetros, estimadores e valores estimados</h2>
<p>Considere o deslocamento de uma partícula no vácuo, em superfície sem atrito. Aprendemos cedo que a velocidade da partícula num instante de tempo <span class="math inline">\(t\)</span>, <span class="math inline">\(v_t\)</span>, é dada por <span class="math inline">\(v_t=v_0+at\)</span>, onde <span class="math inline">\(v_0\)</span> é a velocidade inicial da partícula, <span class="math inline">\(a&gt;0\)</span> é a aceleração aplicada na partícula, neste caso assumida constante. Neste modelo idealizado, a velocidade de uma partícula é uma função linear do tempo, cujo gráfico é apresentado na Figura <a href="rev.html#fig:gfig1">2.5</a>(a).</p>
<p>Um grupo de pesquisadores realizou o seguinte experimento: numa superfície lisa, porém não absolutamente sem atrito, ao ar livre (isto é, na presença de vento, partículas de poeira, etc.) uma partícula foi acelerada à uma determinada aceleração desconhecida, mas constante em cada repetição do experimento, à partir de uma velocidade inicial desconhecida, mas também constante em cada repetição do experimento. Após um determinado tempo <span class="math inline">\(t\)</span> a velocidade da partícula foi medida. Como resultados obtemos pares <span class="math inline">\((v_i,t_i)\)</span> representando a <span class="math inline">\(i\)</span>-ésima observação da velocidade da partícula, medida no tempo <span class="math inline">\(t_i\)</span>. Os resultados estão apresentados na Figura <a href="rev.html#fig:gfig1">2.5</a>(b). Nosso interesse é determinar a velocidade inicial da partícula e a aceleração, que são chamados de parâmetros populacionais. Note que devido às condições não serem ideais, os dados não estão perfeitamente alinhados em uma reta como o estipulado na teoria, mas estão aproximadamente alinhados. Os desvios da reta “esperada” podem ser interpretados como sendo aleatórios, e são devidos aos vários fatores que estão fora de nosso controle, como atrito, vento, partículas em suspensão no ar, etc, fatores que estão em desalinho com a teoria.</p>
<p>Para estimar os parâmetros <span class="math inline">\(a\)</span> e <span class="math inline">\(v_0\)</span>, que denotaremos por <span class="math inline">\(\hat{a}\)</span> e <span class="math inline">\(\hat{v_0}\)</span>, podemos utilizar os estimadores de Mínimos Quadráticos Ordinários que conhecemos, neste caso, dados por (mais detalhes serão fornecidos adiante)
<span class="math display">\[\hat a=\frac{\sum_{i=1}^n(v_i-\bar v)(t_i-\bar t)}{\sum_{i=1}^n(t_i-\bar t)^2} \quad\mbox{ e }\quad \hat{v_0}=\bar{v}-\hat{a}\bar{t},\]</span>
onde <span class="math inline">\(\bar{v}\)</span> denota a média das velocidades e <span class="math inline">\(\bar{t}\)</span> denota a média dos tempos observados. Note que, fornecidos os dados para o estimador, ele retorna dois valores sendo eles a estimativa dos parâmetros <span class="math inline">\(a\)</span> e <span class="math inline">\(v_0\)</span> baseados nos dados. Note que mudando os dados, o estimador continua sendo o mesmo, mas os valores retornados por ele, as estimativas, mudarão. À partir dessas estimativas obtemos a reta apresentada na Figura <a href="rev.html#fig:gfig1">2.5</a>(c)</p>
<p>Na resolução do problema aparecem 3 objetos eminentemente diferentes, cada um deles fundamental na solução do problema e que devem ser entendidos com clareza. Primeiramente temos os parâmetros populacionais, que são os valores de interesse, mas que nos são desconhecidos. Baseado numa amostra, gostaríamos, de alguma forma identificar, esses parâmetros. Segundo temos um estimador, que é uma função dos dados. Quando alimentado de dados estes estimadores retornam valores. Os valores retornados pelo estimador compreendem o terceiro objeto mencionado: são os valores estimados dos parâmetros populacionais.</p>
<p>Esta distinção entre parâmetro, estimador e valor estimado é essencial e está no coração das aplicações de estatística à dados reais.</p>
<div class="figure"><span id="fig:gfig1"></span>
<img src="Figuras/gfig1a.png" alt="Partícula" width="30%" /><img src="Figuras/gfig1b.png" alt="Partícula" width="30%" /><img src="Figuras/gfig1c.png" alt="Partícula" width="30%" />
<p class="caption">
Figura 2.5: Partícula
</p>
</div>
</div>
<div id="propriedades-de-variáveis-aleatórias" class="section level2">
<h2><span class="header-section-number">2.5</span> Propriedades de variáveis aleatórias</h2>
<div id="média-valor-esperado-ou-esperança-matemática" class="section level3">
<h3><span class="header-section-number">2.5.1</span> Média, valor esperado ou esperança matemática</h3>
<p>A Média ou valor esperado, ou ainda a esperança matemática de uma variável aleatória representa o valor médio assumido pela variável em questão. Esta pode ser interpretada como a média ponderada de cada valor assumido pela variável ponderado pela sua probabilidade de ocorrência.</p>

<div class="definition">
<span id="def:defmean" class="definition"><strong>Definição 2.5  (Média, valor esperado ou esperança matemática de variáveis aleatórias discretas.)  </strong></span>Suponha que <span class="math inline">\(X\)</span> seja uma variável aleatória discreta assumindo <span class="math inline">\(n\)</span> valores diferentes <span class="math inline">\(x_1,\cdots x_n\)</span> com probabilidades <span class="math inline">\(p_1,\cdots,p_n\)</span>, respectivamente. Então a média, ou valor esperado ou anda a esperança da variável <span class="math inline">\(X\)</span> é definida por
<span class="math display">\[\mathbb{E}(X)=x_1p_1+x_2p_2+\cdots+x_np_n=\sum_{i=1}^nx_ip_i.\]</span>
</div>

<p>Observe que, no caso discreto, a esperança de uma variável <span class="math inline">\(X\)</span> nada mais é do que a média ponderada de cada valor assumido pela variável pela sua probabilidade de ocorrência.</p>

<div class="example">
<span id="exm:exemploEspdado" class="example"><strong>Exemplo 2.5  </strong></span>Seja <span class="math inline">\(X\)</span> o valor da face superior obtida no lançamento de um
dado equilibrado. Neste caso temos
<span class="math inline">\(P(X=1)=P(X=2)=P(X=3)=P(X=4)=P(X=5)=P(X=6)=\frac{1}{6}\)</span>, ou seja
<span class="math inline">\(p_1=p_2=p_3=p_4=p_5=p_6=\frac{1}{6}\)</span>. Segue que
<span class="math display">\[\begin{eqnarray*}
\mathbb{E}(X)&amp;=&amp;\sum_{i=1}^6p_ix_i= \frac{1}{6}.1+\frac{1}{6}.2+\frac{1}{6}.3+\frac{1}{6}.4+\frac{1}{6}.5+\frac{1}{6}.6\\
     &amp;=&amp;\frac{1}{6}(1+2+3+4+5+6)=\frac{1}{6}.\frac{6(6+1)}{2}\\
     &amp;=&amp;\frac{21}{6}=\frac{7}{2}=3,5.
\end{eqnarray*}\]</span>
O valor 3,5 obtido no resultado deve ser interpretado da seguinte forma: se jogarmos um dado equilibrado um número grande de vezes e calcularmos a média dos valores obtidos, ele será próximo à 3,5. De fato, se fosse possível repertir o experimento um número infinito de vezes, a média dos resultados convergiria para 3,5.
</div>

<p><br></p>

<div class="definition">
<span id="def:defexper" class="definition"><strong>Definição 2.6  (Valor Esperado de <span class="math inline">\(g(X)\)</span>.)  </strong></span>Seja <span class="math inline">\(X\)</span> uma variável aleatória discreta assumindo <span class="math inline">\(n\)</span> valores diferentes <span class="math inline">\(x_1,\cdots x_n\)</span> com probabilidades <span class="math inline">\(p_1,\cdots,p_n\)</span>, respectivamente. Seja <span class="math inline">\(g\)</span> uma função definida na imagem da variável aleatória de <span class="math inline">\(X\)</span>. Então <span class="math inline">\(\mathbb{E}(g(X))\)</span> é dado por
<span class="math display">\[\mathbb{E}(g(X))=g(x_1)p_1+ \cdots + g(x_n)p_n=\sum_{i=1}^n g(x_i)p_i.\]</span>
</div>

<p><br></p>

<div class="example">
<span id="exm:exemploEspdado2" class="example"><strong>Exemplo 2.6  </strong></span>Para o Exemplo  considere
<span class="math inline">\(g(X)=X^2.\)</span> Obtemos
<span class="math display">\[\begin{eqnarray*}
\mathbb{E}(X^2)&amp;=&amp;\sum_{i=1}^6p_ix_i^2= \frac{1}{6}.1+\frac{1}{6}.4+\frac{1}{6}.9+\frac{1}{6}.16+\frac{1}{6}.25+\frac{1}{6}.36\\
     &amp;=&amp;\frac{1}{6}(1+4+9+16+25+36)=\frac{1}{6}.\frac{6(6+1)(12+1)}{6}\\
     &amp;=&amp;\frac{91}{6}=15,16666.
\end{eqnarray*}\]</span>
Note que <span class="math inline">\(\mathbb{E}(X^2)\neq \mathbb{E}(X)^2\)</span>.
</div>

<p><br></p>

<div class="definition">
<span id="def:defexc" class="definition"><strong>Definição 2.7  (Esperança de variáveis aleatórias contínuas,)  </strong></span>Supondo que <span class="math inline">\(X\)</span> seja uma variável aleatória contínua com função de densidade de probabilidade <span class="math inline">\(f\)</span>, definimos a esperança de <span class="math inline">\(X\)</span> por
<span class="math display">\[\mathbb{E}(X)=\int_{-\infty}^{\infty}xf(x)dx.\]</span>
O valor esperado de uma função integrável qualquer de <span class="math inline">\(X\)</span>, digamos <span class="math inline">\(g(X)\)</span> é definido por
<span class="math display">\[\mathbb{E}(g(X))=\int_{-\infty}^{\infty}g(x)f(x)dx.\]</span>
</div>


<div class="example">
<span id="exm:exmmeanNormal" class="example"><strong>Exemplo 2.7  </strong></span> Se <span class="math inline">\(X\sim N(\mu,\sigma^2)\)</span>, então <span class="math inline">\(\mathbb{E}(X)=\mu\)</span>, como pode ser facilmente computado.
</div>

</div>
<div id="propriedades-da-esperança" class="section level3">
<h3><span class="header-section-number">2.5.2</span> Propriedades da Esperança</h3>
<p>No que segue, assumimos que <span class="math inline">\(X, Y\)</span> são variáveis aleatórias e <span class="math inline">\(a, b, c\)</span> são constantes reais.</p>
<ul>
<li><p><strong>E1</strong> - <span class="math inline">\(\mathbb{E}(a) = a;\)</span></p></li>
<li><p><strong>E2</strong> - <span class="math inline">\(\mathbb{E}(a + X) = a + \mathbb{E}(X);\)</span></p></li>
<li><p><strong>E3</strong> - <span class="math inline">\(\mathbb{E}(b X) = b \mathbb{E}(X);\)</span></p></li>
<li><p><strong>E4</strong> - <span class="math inline">\(\mathbb{E}(a + b X) = a + b \mathbb{E}(X);\)</span></p></li>
<li><p><strong>E5</strong> - <span class="math inline">\(\mathbb{E}(X + Y) = \mathbb{E}(X) + \mathbb{E}(Y);\)</span></p></li>
<li><p><strong>E6</strong> - <span class="math inline">\(\mathbb{E}(a + bX + cY) = a + b \mathbb{E}(X) + c \mathbb{E}(Y);\)</span></p></li>
</ul>
<p>Estas propriedades podem ser generalizadas para qualquer número de variáveis aleatórias.</p>
<p>Em particular, segue a esperança de uma combinação linear de variáveis aleatórias
é a combinação linear das suas esperança, isto é, se <span class="math inline">\(X_1,\cdots,X_n\)</span> são variáveis aleatórias e <span class="math inline">\(a_1,\cdots,a_n\)</span> são constantes reais,</p>
<ul>
<li><strong>E7</strong> - <span class="math inline">\(\displaystyle{\mathbb{E}\bigg(\sum_{i=1}^na_iX_i\bigg)= \sum_{i=1}^na_i\mathbb{E}(X_i)}.\)</span></li>
</ul>
<p>Por esse motivo, a função <span class="math inline">\(\mathbb{E}(\cdot)\)</span> que associa a cada variável aleatória o seu valor esperado é um , chamado de .</p>
<p>Em geral, temos que <span class="math inline">\(\mathbb{E}(X Y) \neq \mathbb{E}(X) \mathbb{E}(Y)\)</span>. Porém, no caso particular em que <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span> são variáveis aleatórias independentes, a igualdade é válida, isto é,
<span class="math display">\[\mathbb{E}(X Y) = \mathbb{E}(X) \mathbb{E}(Y)\quad \mbox{se, e somente se, $X$ e $Y$ são independentes}.\]</span></p>
</div>
<div id="variância" class="section level3">
<h3><span class="header-section-number">2.5.3</span> Variância</h3>
<p>Seja <span class="math inline">\(X\)</span> uma variável aleatória (contínua ou discreta)e defina <span class="math inline">\(\mu = \mathbb{E}(X)\)</span>. Então a variância de <span class="math inline">\(X\)</span> é definida por</p>
<p><span class="math display" id="eq:var">\[\begin{equation}
\mbox{Var}(X)={\mathbb{E}}[(X-\mu)^2)]=\mathbb{E}(X^2)-[\mathbb{E}(X)]^2.
\tag{2.2}
\end{equation}\]</span></p>
<p>Podemos interpretar a variância como sendo o valor esperado do quadrado do desvio de <span class="math inline">\(X\)</span> da sua própria média. Em linguagem comum isto pode ser expresso como . É assim a . A variância da variável aleatória <span class="math inline">\(X\)</span> é geralmente designada por <span class="math inline">\(\mbox{Var}(X)\)</span>, <span class="math inline">\(\sigma^2_X\)</span>, ou simplesmente <span class="math inline">\(\sigma^2\)</span>. A variância é uma medida de dispersão dos dados e sua unidade é a unidade dos dados elevada ao quadrado. Lembramos que a raiz quadrada positiva da variância determina o chamado desvio padrão de <span class="math inline">\(X\)</span>.</p>
</div>
<div id="covariância" class="section level3">
<h3><span class="header-section-number">2.5.4</span> Covariância</h3>
<p>A covariância entre duas variáveis aleatórias <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span> com <span class="math inline">\({\mathbb{E}}(X)=\mu_{X}\)</span> e
<span class="math inline">\({\mathbb{E}}(Y)=\mu_{Y}\)</span> é definida por
<span class="math display">\[\mbox{Cov}(X, Y) = {\mathbb{E}}[(X - \mu_{X}) (Y - \mu_{Y})].\]</span></p>
<p>Desenvolvendo a expressão para a covariância, temos:
<span class="math display">\[\begin{align*}
\mbox{Cov}(X, Y) &amp;= {\mathbb{E}}\big[(X - \mu_{X}) (Y - \mu_{Y})\big]\\
&amp;={\mathbb{E}}\big[(X - \mathbb{E}(X)) (Y - {\mathbb{E}}(Y))\big]\\
&amp;={\mathbb{E}}\big[XY - X{\mathbb{E}}(Y) - Y{\mathbb{E}}(X) + {\mathbb{E}}(X){\mathbb{E}}(Y)\big].
\end{align*}\]</span>
Usando a propriedade de que a esperança da soma entre duas variáveis aleatórias é igual a soma das esperanças, segue que</p>
<p><span class="math display" id="eq:aquela">\[\begin{align}
\mbox{Cov}(X, Y) &amp;= {\mathbb{E}}(XY) - {\mathbb{E}}\big[X{\mathbb{E}}(Y)\big] - {\mathbb{E}}\big[Y{\mathbb{E}}(X)\big] +  {\mathbb{E}}\big[{\mathbb{E}}(X){\mathbb{E}}(Y)\big]\nonumber\\
&amp;= {\mathbb{E}}(XY) - {\mathbb{E}}(Y){\mathbb{E}}(X) - {\mathbb{E}}(X){\mathbb{E}}(Y) + {\mathbb{E}}(X){\mathbb{E}}(Y)\nonumber\\
&amp;=\mathbb{E}(X Y) - \mathbb{E}(X) \mathbb{E}(Y)
\tag{2.3}
\end{align}\]</span>
Note que quando <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span> são independentes, temos que <span class="math inline">\(\mathbb{E}(XY)=\mathbb{E}(X)\mathbb{E}(Y)\)</span> de onde segue que <span class="math inline">\(\mbox{Cov}(X,Y)=0\)</span>. A recíproca, porém, não é verdadeira pois existem exemplos de variáveis dependentes que possuem covariância nula. Observe ainda que da expressão <a href="rev.html#eq:aquela">(2.3)</a> podemos concluir que a covariância é uma forma de medir o quão “distante” <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span> estão de ser independentes.</p>
</div>
<div id="correlação" class="section level3">
<h3><span class="header-section-number">2.5.5</span> Correlação</h3>
<p>A correlação, também chamada de coeficiente de correlação, indica a força e a direção do relacionamento linear entre duas variáveis aleatórias, se existir. A correlação entre duas variáveis <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span> com <span class="math inline">\(0&lt;\mbox{Var}(X)&lt;\infty\)</span> e <span class="math inline">\(0&lt;\mbox{Var}(Y)&lt;\infty\)</span>, denotado por <span class="math inline">\(\mbox{Cor}(X,Y)\)</span> ou <span class="math inline">\(\rho_{_{X,Y}}\)</span>, é definida como
<span class="math display">\[\mbox{Cor}(X,Y)=\rho_{_{X,Y}}=\frac{\mbox{Cov}(X,Y)}{  \sqrt{\mbox{Var}(X)\mbox{Var}(Y)}}=\frac{\mathbb{E}(XY)-\mathbb{E}(X)\mathbb{E}(Y)}{\sqrt{\mathbb{E}(X^2)-\mathbb{E}^2(X)}~\sqrt{\mathbb{E}(Y^2)-\mathbb{E}^2(Y)}}.\]</span>
Note que a correlação entre <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span> nada mais é do que a covariância entre <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span> normalizada por seus desvios padrões. Esta normalização acaba dando à correlação uma interpretabilidade ausente na covariância como veremos a seguir.</p>
<p>Observe ainda que, quando <span class="math inline">\(\mbox{Cov}(X,Y)=0\)</span>, temos <span class="math inline">\(\mbox{Cor}(X,Y)=0\)</span> também e <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span> são ditos ser variáveis não-correlacionadas.</p>
</div>
<div id="propriedades-da-variância-covariância-e-correlação" class="section level3">
<h3><span class="header-section-number">2.5.6</span> Propriedades da variância, covariância e correlação</h3>
<p>Se <span class="math inline">\(a\)</span> e <span class="math inline">\(b\)</span> forem constantes reais e <span class="math inline">\(X\)</span> uma variável aleatória cuja variância está definida, então:</p>
<ul>
<li><p><strong>V1</strong> - <span class="math inline">\(\mbox{Var}(aX+b)=a^2\mbox{Var}(X);\)</span></p></li>
<li><p><strong>V2</strong> - <span class="math inline">\(\mbox{Var}(X+Y) =\mbox{Var}(X) + \mbox{Var}(Y)+ 2 \mbox{Cov}(X, Y).\)</span></p></li>
</ul>
<p>Da propriedade V1 segue que a variância de uma constante é zero. Além disso, se a variância de uma variável aleatória é zero, então esta variável assume um único valor com probabilidade 1. Da propriedade V2 segue que se <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span> são não-correlacionados, então a variância da soma é a soma das variâncias.</p>
<p>Suponha agora que <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span> são variáveis aleatórias e <span class="math inline">\(a\)</span>, <span class="math inline">\(b\)</span>, <span class="math inline">\(c\)</span> e <span class="math inline">\(d\)</span> são constantes reais. Então</p>
<ul>
<li><p><strong>Cv1</strong> - <span class="math inline">\(\mbox{Cov}(X, X) = \mbox{Var}(X)\)</span>;</p></li>
<li><p><strong>Cv2</strong> - <span class="math inline">\(\mbox{Cov}(X, Y) = \mbox{Cov}(Y,X)\)</span>;</p></li>
<li><p><strong>Cv3</strong> - <span class="math inline">\(\mbox{Cov}(aX + b, cY + d) = ac\mbox{Cov}(X, Y)\)</span>;</p></li>
<li><p><strong>Cv4</strong> - <span class="math inline">\(\displaystyle{\mbox{Cov}\bigg(\sum_{i=1}^n{X_i}, \sum_{j=1}^m{Y_j}\bigg)= \sum_{i=1}^n\sum_{j=1}^m\mbox{Cov}(X_i, Y_j)}\)</span>.</p></li>
</ul>
<p>Como mencionado anteriormente, se <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span> são independentes, então <span class="math inline">\(\mbox{Cov}(X,Y)=0\)</span>.</p>
<p>A correlação, por sua vez, possui as seguintes propriedades:</p>
<ul>
<li><p><strong>Cr1</strong> - <span class="math inline">\(\left| \mbox{Cor}(X,Y)\right|\leq 1\)</span>;</p></li>
<li><p><strong>Cr2</strong> - <span class="math inline">\(\mbox{Cor}(X,Y) = 1\)</span> se, e somente se, <span class="math inline">\(X\)</span> é diretamente proporcional a <span class="math inline">\(Y\)</span> no sentido de que <span class="math inline">\(X=a+bY\)</span> para <span class="math inline">\(a\in\mathbb{R}\)</span> e <span class="math inline">\(b&gt;0\)</span>;</p></li>
<li><p><strong>Cr3</strong> - <span class="math inline">\(\mbox{Cor}(X, Y) = -1\)</span> se, e somente se, <span class="math inline">\(X\)</span> é inversamente proporcional a <span class="math inline">\(Y\)</span> no sentido de que <span class="math inline">\(X=a+bY\)</span> para <span class="math inline">\(a\in\mathbb{R}\)</span> e <span class="math inline">\(b&lt;0\)</span>;</p></li>
<li><p><strong>Cr4</strong> - <span class="math inline">\(\mbox{Cor}(X, Y) = \mbox{Cor}(Y,X)\)</span>;</p></li>
<li><p><strong>Cr5</strong> - <span class="math inline">\(\mbox{Cor}(aX + b, cY + d) = \mathrm{sign}(ac)\mbox{Cor}(X, Y)\)</span>, onde a função sign<span class="math inline">\((x)\)</span> é a função sinal de <span class="math inline">\(x\)</span>, sendo igual a <span class="math inline">\(-1\)</span>, se <span class="math inline">\(x&lt;0\)</span>, 1 se <span class="math inline">\(x&gt;0\)</span> e <span class="math inline">\(0\)</span> se <span class="math inline">\(x=0\)</span>;</p></li>
<li><p><strong>Cr6</strong> - Se <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span> são independentes, então <span class="math inline">\(\mbox{Cor}(X,Y)=0\)</span>. A reciproca, porém, não é verdadeira.</p></li>
</ul>
</div>
</div>
<div id="estimadores" class="section level2">
<h2><span class="header-section-number">2.6</span> Estimadores</h2>
<p>Dada uma amostra <span class="math inline">\(x_1,x_2,\cdots,x_n\)</span> de uma variável aleatória <span class="math inline">\(X\)</span>, o estimador de <span class="math inline">\(\mathbb{E}(X)\)</span> é simplesmente a média aritmética dos dados:</p>
<p><span class="math display">\[ \overline{X}=\frac{1}{n}\sum_{i=1}^{n}x_i.\]</span></p>
<p>Com relação à variância de <span class="math inline">\(X\)</span>, existem dois estimadores muito utilizados na prática. O estimador da variância de <span class="math inline">\(X\)</span> obtido pelo método de máxima verossimilhança é dado por</p>
<p><span class="math display">\[\hat{\sigma}_X^2=\frac{1}{n}\sum_{i=1}^{n}(x_i-\overline{x})^2=\frac1n\bigg(\sum_{i=1}^nx_i^2-n\overline{x}^2\bigg).\]</span></p>
<p>Pode-se mostrar que, embora consistente, este estimador é viesado em amostras finitas. Um estimador consistente e não-viesado em amostras finitas é dado por
<span class="math display">\[ {S}_X^2=\frac{1}{n-1}\sum_{i=1}^{n}(x_i-\overline{x})^2=\frac1{n-1}\bigg(\sum_{i=1}^nx_i^2-n\overline{x}^2\bigg).\]</span></p>
<p>Observe que para <span class="math inline">\(n\)</span> grandes, a diferença entre os estimadores <span class="math inline">\(\hat\sigma^2\)</span> e <span class="math inline">\(S^2\)</span> é irrelevante. Em amostras pequenas, porém, o estimador <span class="math inline">\(S^2\)</span> apresenta uma performance melhor.</p>
<p>Seja <span class="math inline">\(x_1,x_2,\cdots,x_n\)</span> e <span class="math inline">\(y_1,y_2,\cdots,y_n\)</span> amostras aleatórias das variáveis
aleatórias <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span>. Então um estimador para a covariância entre <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span> é dado por
<span class="math display">\[\hat{ \gamma}_{_{X,Y}}=\frac{1}{n-1}\sum_{i=1}^{n}(x_i-\overline{x})(y_i-\overline{y})=\frac1{n-1}\bigg(\sum_{i=1}^{n}x_iy_i-n\overline{x}\overline{y}\bigg).\]</span></p>
<div class="line-block">Um estimador para a correlação entre <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span> é dado por</div>
<p><span class="math display">\[\hat{\rho}_{_{X,Y}} = \frac{\hat{ \gamma}_{_{X,Y}}}{S_XS_Y}.\]</span></p>
<div id="propriedades-dos-estimadores" class="section level3">
<h3><span class="header-section-number">2.6.1</span> Propriedades dos estimadores</h3>
<p>Dado que temos alguns estimadores definidos acima, é interessante estudar algumas
das propriedades qualitativas dos estimadores que nos permitam determinar qual estimador é <em>“bom”</em> e qual não é. É também importante definir critérios para compar diversos estimadores.</p>
</div>
<div id="vícioviés" class="section level3">
<h3><span class="header-section-number">2.6.2</span> Vício/Viés</h3>
<p>Seja <span class="math inline">\(\hat{\theta}\)</span> um estimador do parâmetro <span class="math inline">\(\theta\)</span>. o vício/viés (bias, em inglês) é definido como</p>
<p><span class="math display" id="eq:vies">\[\begin{equation}
 b(\hat{\theta})=\mathbb{E}(\hat{\theta})-\theta.
 \tag{2.4}
\end{equation}\]</span></p>
<p>Se <span class="math inline">\(b(\hat\theta)=0\)</span> segue que <span class="math inline">\(\mathbb{E}(\hat{\theta})-\theta\)</span> e, neste caso, dizemos que <span class="math inline">\(\hat\theta\)</span> é não-viciado ou não-viesado para o parâmetro <span class="math inline">\(\theta\)</span>.</p>
</div>
<div id="consistência" class="section level3">
<h3><span class="header-section-number">2.6.3</span> Consistência</h3>
<p>Em estatística, uma seqüência de estimadores para o parâmetro <span class="math inline">\(\theta\)</span> é dito ser consistente (ou assintoticamente consistente) se esta sequência converge em probabilidade para <span class="math inline">\(\theta\)</span>. Isso significa que as distribuições dos estimadores tornar-se mais e mais concentrados perto do verdadeiro valor do parâmetro a ser estimado, de modo que a probabilidade do estimador ser arbitrariamente perto <span class="math inline">\(\theta\)</span> converge para um.</p>
</div>
<div id="eficiência" class="section level3">
<h3><span class="header-section-number">2.6.4</span> Eficiência</h3>
<p>Um estimador de <span class="math inline">\(\theta\)</span> é dito ser eficiente se for não viesado e sua variância for menor ou igual a variância de qualquer outro estimador <span class="math inline">\(\hat{\theta}\)</span>, ou seja,
<span class="math display">\[
\mbox{Var}(\hat{\theta}_0)\leq \mbox{Var}(\hat{\theta}),\,\,\,\,
        \mbox{para  qualquer outro estimador}\,\,\,\,\, \hat{\theta}\,\,\, \mbox{ de }\theta.
\]</span></p>
<p>Na figura abaixo podemos observar a diferença entre vício e eficiência. Estes conceitos estão relacionados à média e à variância, respectivamente.</p>
<div class="figure" style="text-align: center"><span id="fig:Propest"></span>
<img src="Figuras/vicio_var.png" alt="Diferença entre vício e eficiência." width="80%" />
<p class="caption">
Figura 2.6: Diferença entre vício e eficiência.
</p>
</div>
</div>
<div id="erro-quadrático-médio-eqm" class="section level3">
<h3><span class="header-section-number">2.6.5</span> Erro quadrático médio (EQM)</h3>
<p>O erro quadrático médio de um estimador <span class="math inline">\(\hat{\theta}\)</span> de <span class="math inline">\(\theta\)</span> é definido como
<span class="math display" id="eq:eqm">\[\begin{equation}
EQM(\hat\theta) = \mathbb{E}(\hat{\theta} - \theta)^2.
\tag{2.5}
\end{equation}\]</span></p>
<p>Podemos reescrever esta ultima expressão como
<span class="math display">\[EQM (\hat\theta) = \mbox{Var}(\theta) + [\mathbb{E}(\theta) - \theta]^2 =\mbox{Var}(\hat{\theta})+b(\hat{\theta}).\]</span></p>
<p>Assim, o erro quadrático médio é definido como a variância do estimador mais o quadrado do seu viés. Podemos entender o EQM como sendo uma medida da performance de um estimador em relação ao seu vício e variância. Note que EQM<span class="math inline">\((\theta)=\mbox{Var}(\theta)\)</span> sempre que o estimador for não-viciado.</p>
</div>
<div id="vício-versus-vvariância-mínima" class="section level3">
<h3><span class="header-section-number">2.6.6</span> Vício versus Vvariância mínima</h3>
<p>O erro quadrático médio utilizado na comparação entre um ou mais estimadores para um
mesmo parâmetro <span class="math inline">\(\theta\)</span>. Podemos observar de <a href="rev.html#eq:eqm">(2.5)</a> que, no cálculo do EQM, existe um balanço entre vício e variância. Naturalmente, estimadores eficientes apresentarão um EQM mínimo dentre os estimadores não-viciados de <span class="math inline">\(\theta\)</span>. Muitas vezes, porém, pode ser mais vantajoso do ponto de vista prático a utilização de um estimador viciado mas com variância pequena em detrimento a um estimador de maior variância, mas que seja não-viciado. Isto ocorre por que se a variância de um estimado é muito grande, é grande a chance de uma estimativa esteja longe do verdadeiro valor do parâmetro, mesmo que o estimador seja não-viciado. Este é um ponto importante a ser observado quando da escolha de um estimador para um determinado
problema.</p>
</div>
<div id="método-de-mínimos-quadrados-mqo" class="section level3">
<h3><span class="header-section-number">2.6.7</span> Método de mínimos quadrados (MQO)</h3>
<p>Considere o modelo
<span class="math display">\[Y={\alpha}+{\beta}X+U\]</span>
onde <span class="math inline">\(Y\)</span> é a variável dependente, <span class="math inline">\(X\)</span> é a vaiável independente e <span class="math inline">\(U\)</span> denota o termo de erro do modelo. Suponhamos que temos uma amostra <span class="math inline">\((x_1,y_1),\cdots,(x_n,y_n)\)</span> provindo deste modelo.</p>
<div id="qual-critério-devo-utilizar-para-obter-os-estimadores-dos-parâmetros-alpha-e-beta" class="section level4">
<h4><span class="header-section-number">2.6.7.1</span> Qual critério devo utilizar para obter os estimadores dos parâmetros <span class="math inline">\(\alpha\)</span> e <span class="math inline">\(\beta\)</span>?</h4>
<p>Podemos minimizar:</p>
<ul>
<li><p>Soma dos erros: não é um bom critério pois pode anular positivos e negativos.</p></li>
<li><p>Soma Absoluta dos Resíduos: é um critério válido e intuitivo, porém seu estudo é de alta complexidade. Devido a isso, o estimador obtido por este critério, denominado LAD (Least Absolute Deviations), é pouco utilizado na prática.</p></li>
<li><p>Soma dos Quadrados dos Erros: possui propriedades estatísticas de simples utilização e interpretação o que o tornam bastante atrativo. É este o critério que dá origem ao estimador de mínimos quadráticos ordinários (MQO).</p></li>
</ul>
<p>Utilizando a soma dos quadrados dos erros como critério, devemos resolver o seguinte problema de optimização:
<span class="math display" id="eq:gl1">\[\begin{equation}
\min_{\{\alpha,\beta\}}\bigg\{\sum_{i=1}^n u_i^2\bigg\} = \min_{\{\widehat{\alpha},\widehat{\beta}\}}\bigg\{\sum_{i=1}^n(y_i-\alpha-\beta x_i)^2\bigg\}.
\tag{2.6}
\end{equation}\]</span></p>
<p>As  (CPO’s) são obtidas difereciando-se o argumento do lado direito de <a href="rev.html#eq:gl1">(2.6)</a> em relação à <span class="math inline">\(\alpha\)</span> e <span class="math inline">\(\beta\)</span>. Em <span class="math inline">\(\alpha\)</span>, a solução do problema de optimização será o valor <span class="math inline">\(\hat{\alpha}\in \mathbb{R}\)</span> que satisfaz</p>
<p><span class="math display">\[-2\sum_{i=1}^n(y_i-\widehat{\alpha}-\widehat{\beta}x_i)=0 \ \ \Longrightarrow \ \ \sum_{i=1}^n \widehat{u}_i=0.\]</span></p>
<p>Esta CPO nos mostra que a escolha do intercepto ótimo implica que a soma dos resíduos será zero. Continuando com essa CPO</p>
<p><span class="math display" id="eq:CPOalpha">\[\begin{align}
\nonumber  \sum_{i=1}^n(y_i-\widehat{\alpha}-\widehat{\beta}x_i)=0 &amp;\Longleftrightarrow \ \ n\overline{y}-n\widehat{\alpha}-\widehat{\beta}n\overline{x} = 0 \\
%\nonumber  \sum_{i=1}^ny_i-\sum_{i=1}^n\widehat{\alpha}-\sum_{i=1}^n\widehat{\beta}x_i &amp;=&amp; 0 \\
&amp;\Longleftrightarrow \ \  \widehat{\alpha}_{MQO} =  \overline{y}-\widehat{\beta}\overline{x}.
\tag{2.7}
\end{align}\]</span></p>
<p>Assim, o estimador de MQO do intercepto <span class="math inline">\(\alpha\)</span> é dado por <a href="rev.html#eq:CPOalpha">(2.7)</a>.</p>
<p>Difereciando-se o argumento do lado direito de <a href="rev.html#eq:gl1">(2.6)</a> em relação à <span class="math inline">\(\beta\)</span> obtemos que a solução do problema de optimização será o valor <span class="math inline">\(\hat{\beta}\in \mathbb{R}\)</span> que satisfaz</p>
<p><span class="math display">\[\begin{align*}
\nonumber  &amp;\sum_{i=1}^n(y_i-\widehat{\alpha}-\widehat{\beta}x_i)^2 = 0  \ \ &amp;\Longleftrightarrow \ \ \sum_{i=1}^ny_ix_i - \widehat{\alpha}\sum_{i=1}^nx_i-\widehat{\beta}\sum_{i=1}^nx_i^2 =0\\
\nonumber  \ \ &amp;\Longleftrightarrow \ \  \sum_{i=1}^ny_ix_i = (\overline{y}-\widehat{\beta}\overline{x})\sum_{i=1}^nx_i+\widehat{\beta}\sum_{i=1}^nx_i^2 \\
\ \ &amp;\Longleftrightarrow \ \ \sum_{i=1}^ny_ix_i = \overline{y}\sum_{i=1}^nx_i+\widehat{\beta}\bigg(\sum_{i=1}^nx_i^2-\overline{x}\sum_{i=1}^nx_i\bigg), \\
\end{align*}\]</span>
onde a última igualdade obtém-se dividindo-se o numerador e denominador por <span class="math inline">\(n-1\)</span>.</p>
</div>
</div>
<div id="regressão-liner-múltipla-rml" class="section level3">
<h3><span class="header-section-number">2.6.8</span> Regressão liner múltipla (RML)</h3>
<p>Considere o modelo de regressão linear múltipla</p>
<p><span class="math display">\[y_i=\beta_0+\beta_1x_{1i}+\beta_2x_{2i}+\cdots+\beta_kx_{ki}+u_i\]</span>
em que temos <span class="math inline">\(k\)</span> variáveis explicativas <span class="math inline">\(x_1,\cdots,x_k\)</span>.
Definindo</p>
<p><span class="math display">\[Y=\left[ \begin{array}{c}
   y_{1} \\
   y_{2} \\
  \vdots \\
   y_{n} \\
         \end{array}
  \right], \qquad X=\left[
\begin{array}{ccccc}
  1 &amp; x_{11} &amp; x_{21} &amp; \cdots &amp; x_{k1} \\
  1 &amp; x_{12} &amp; x_{22} &amp; \cdots &amp; x_{k2} \\
  \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
  1 &amp; x_{1n} &amp; x_{2n} &amp; \cdots &amp; x_{kn} \\
\end{array}
\right], \qquad
\]</span>
e
<span class="math display">\[
{\beta}=\left[ \begin{array}{c}
   \beta_0 \\
   \beta_1 \\
  \vdots \\
   \beta_k \\
         \end{array}
  \right]
\qquad U=
\left[ \begin{array}{c}
   u_{1} \\
   u_{2} \\
  \vdots \\
   u_{n} \\
         \end{array}
  \right] \]</span>
obtemos o modelo de regressão em forma matricial <span class="math inline">\(Y=X{ \beta}+U\)</span>. A matriz <span class="math inline">\(X\)</span> é chamada de matriz de design do modelo. Pode-se mostrar que o estimador de MQO para <span class="math inline">\(\beta\)</span> é dado por:</p>
<p><span class="math display">\[\hat{\beta}=(X&#39;X)^{-1}X&#39;Y.\]</span></p>
</div>
<div id="hipóteses-do-modelo-de-regressão" class="section level3">
<h3><span class="header-section-number">2.6.9</span> Hipóteses do modelo de regressão</h3>
<ul>
<li><strong>Hipótese 1 (Linearidade dos Parâmetros): </strong> A relação entre a variável dependente <span class="math inline">\(Y\)</span> e as explicativas <span class="math inline">\(X_1, \cdots, X_k\)</span> é linear</li>
</ul>
<p><span class="math display">\[Y=\beta_0+\beta_1X_1+\cdots+\beta_kX_{k}+U.\]</span></p>
<p><br></p>

<div class="definition">
<span id="def:defslp" class="definition"><strong>Definição 2.8  </strong></span>Um modelo de regressão é linear nos parâmetros se as CPOs
associadas ao problema de obtenção dos EMQ (Estimadores de MQO)
gerarem um sistema linear nos parâmetros.
</div>

<p><br></p>

<div class="example">
<p><span id="exm:exmslp" class="example"><strong>Exemplo 2.8  </strong></span>Seja o seguinte modelo <span class="math display">\[Y=\alpha+\beta X+U.\]</span> e <span class="math inline">\((x_i,y_i)\)</span>, para <span class="math inline">\(i=1,\cdots,n\)</span>, uma amostra do modelo. De acordo com o que foi visto anteriormente, o problema de optimização a ser resolvido para a obtenção dos estimadores de MQO para <span class="math inline">\(\alpha\)</span> e <span class="math inline">\(\beta\)</span> será</p>
<p><span class="math display">\[\underset{\{\alpha,\beta\}}{\min}\bigg\{\sum_{i=1}^n(y_i-\alpha-\beta x_i)^2\bigg\}.\]</span>
As CPOs serão
<span class="math display">\[\widehat{\alpha}:-2\sum_{i=1}^n(y_i-\widehat{\alpha}-\widehat{\beta}x_i)=0 \quad \Longrightarrow\quad \sum_{i=1}^n y_i=n\widehat{\alpha}+\widehat{\beta}\sum_{i=1}^nx_i\]</span></p>
<p><span class="math display">\[\widehat{\beta}:-2\sum_{i=1}^n(y_i-\widehat{\alpha}-\widehat{\beta}x_i)x_i=0 \quad \Longrightarrow\quad \sum_{i=1}^ny_ix_i=\widehat{\alpha}\sum_{i=1}^nx_i+\widehat{\beta}\sum_{i=1}^nx_i^2\]</span></p>
<span class="math display">\[\left[%
\begin{array}{cc}
  n &amp; \sum_{i=1}^nx_i \\
  \sum_{i=1}^nx_i &amp; \sum_{i=1}^nx_i^2 \\
\end{array}%
\right]\left[%
\begin{array}{c}
  \widehat{\alpha} \\
  \widehat{\beta} \\
\end{array}%
\right]=\left[%
\begin{array}{c}
  \sum_{i=1}^ny_i \\
  \sum_{i=1}^ny_ix_i \\
\end{array}%
\right].\]</span>
Logo é o sistema linear e o modelo é linear nos parâmetros.
</div>

<p><br></p>

<div class="example">
<span id="exm:exmslp2" class="example"><strong>Exemplo 2.9  </strong></span>Seja o seguinte modelo <span class="math display">\[Y=\alpha+\beta X^{\gamma}+U \]</span> e seja <span class="math inline">\((x_i,y_i)\)</span>, para <span class="math inline">\(i=1,\cdots,n\)</span>, uma amostra do modelo. O problema de minimização neste caso resume-se a
<span class="math display">\[\underset{\{\alpha,\beta,\gamma\}}{\min}\bigg\{\sum_{i=1}^n(y_i-\alpha-\beta x_i^{\gamma})^2\bigg\}.\]</span>
A CPO em <span class="math inline">\(\alpha\)</span> é dada por
<span class="math display">\[\alpha:-2\sum_i(y_i-\alpha-\beta x_i^{\gamma})=0,\]</span>
que não é linear por causa do <span class="math inline">\(\gamma\)</span>.
</div>

<p><br></p>

<div class="example">
<p><span id="exm:exmslp3" class="example"><strong>Exemplo 2.10  </strong></span>Seja o seguinte modelo <span class="math display">\[Y=\alpha X_1^{\beta_1}X_2^{\beta_2}e^{U}.\]</span>
Este modelo é claramente não-linear, porém, ao tomarmos o logaritmo obtemos</p>
<span class="math display">\[\ln (Y)=\ln (\alpha)+\beta_1\ln(X_1)+\beta_2\ln(X_2)+U,\]</span>
que é linear nos parâmetros.
</div>

<ul>
<li><strong>Hipótese 2 (Amostragem Aleatória): </strong> Podemos extrair uma amostra aleatória
<span class="math display">\[\{(x_{1i},\cdots,x_{ki},y_i),i=1,\cdots,n\}\]</span> da população.</li>
</ul>
<p><br></p>

<div class="remark">
 <span class="remark"><em>Observação. </em></span> Nos livros-texto esta hipótese é geralmente substituída por uma
hipótese de que <span class="math inline">\(X\)</span> é determinístico (não aleatório) e seus valores podem ser escolhido de antemão.
</div>

<p><br></p>
<ul>
<li><p><strong>Hipótese 3 (Média Condicional Zero): </strong> <span class="math inline">\(\mathbb{E}(U|X)=0\)</span></p></li>
<li><p><strong>Hipótese 4 (Não há Multicolinearidade Perfeita): </strong> As variáveis explicativas <span class="math inline">\(X_1,\cdots,X_k\)</span> são linearmente independentes. Logo, <span class="math inline">\(X_j,j=1,\cdots,k\)</span> não podem ser constantes. Lembrando que o posto de uma matriz <span class="math inline">\(X\)</span> é a dimensão do subspaço gerado pelas colunas da matriz, esta hipótese implica que a matriz de design associada ao modelo,
<span class="math display">\[X=\left[%
\begin{array}{ccccc}
1 &amp; x_{11} &amp; x_{21} &amp; \cdots &amp; x_{k1} \\
1 &amp; x_{12} &amp; x_{22} &amp; \cdots &amp; x_{k2} \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
1 &amp; x_{1n} &amp; x_{2n} &amp; \cdots &amp; x_{kn} \\
\end{array}%
\right]_{n \times (k+1)}\]</span>
tem posto máximo, isto é, posto<span class="math inline">\((X)=k+1\)</span>, pois <span class="math inline">\(n\geq k+1\)</span>. Relembre das propriedades de
álgebra matricial que
<span class="math display">\[\mathrm{posto}(X&#39;X)=\mathrm{posto}(X)=k+1,\]</span>
e assim, <span class="math inline">\((X&#39;X)\)</span> é uma matriz invertível.</p></li>
<li><p><strong>Hipótese 5 (Homocedasticidade): </strong>
Se <span class="math inline">\(U_1,\cdots,U_n\)</span> é a sequência de erros relativa ao modelo linear <span class="math inline">\(Y=X{\beta}+U\)</span> baseado numa amostra de tamanho <span class="math inline">\(n\)</span> do modelo.
Então <span class="math inline">\(\mbox{Var}(U_i|X)=\sigma^2\)</span>, para todo <span class="math inline">\(i\)</span>, ou seja, a variância do erro é constante.</p></li>
<li><p><strong>Hipótese 6 (Ausência de (Auto)Correlação (Serial) Condicional): </strong> <span class="math inline">\(\mbox{Cov}(U_i,U_j|X)=0\)</span>, para todo <span class="math inline">\(i\)</span> e <span class="math inline">\(j\)</span> com <span class="math inline">\(i \neq j\)</span>.</p></li>
<li><p><strong>Hipótese 7 (Normalidade): </strong> <span class="math inline">\(U_i \sim N(0,\sigma^2)\)</span> para todo <span class="math inline">\(i\)</span>. Tal hipótese será necessária para inferência.</p></li>
</ul>
<p><br></p>

<div class="theorem">
<span id="thm:thmGM" class="theorem"><strong>Teorema 2.1  (de Gauss-Markov)  </strong></span>Dentro da classe dos estimadores lineares e não-viesados, e dadas
as hipóteses do MCRL, os EMQs são estimadores que possuem a menor
variância (BLUE - Best Linear Unbiased Estimator).
</div>

</div>
<div id="o-coeficiente-de-dterminação" class="section level3">
<h3><span class="header-section-number">2.6.10</span> O coeficiente de dterminação</h3>
<p>Existe alguma medida que mostre que um determinado modelo apresenta um bom
poder preditivo? Ou seja, se o regressor (<span class="math inline">\(X\)</span>) que eu inclui no
meu modelo explica bem a variável dependente (<span class="math inline">\(Y\)</span>)? Para construirmos tal medida, primeiramente definimos
<span class="math display">\[\begin{eqnarray*}
  \sum_{i=1}^n(y_i^*)^2 &amp;=&amp; \mbox{Soma dos Quadrados Totais ($SQT$)} \\
  \sum_{i=1}^n(\widehat{y}_i^*)^2 &amp;=&amp; \mbox{Soma dos Quadrados Explicados ($SQE$)} \\
  \sum_{i=1}^n\widehat{u}_i^2 &amp;=&amp; \mbox{Soma dos Quadrados dos Resíduos ($SQR$)}
\end{eqnarray*}\]</span>
Pode-se mostrar facilmente que
<span class="math display">\[SQT=SQE+SQR.\]</span>
Dividindo a expressão por <span class="math inline">\(SQT\)</span>, teremos
<span class="math display">\[1=\underbrace{\frac{SQE}{SQT}}_{R^2}+\frac{SQR}{SQT}.\]</span></p>
<p>O <span class="math inline">\(R^2\)</span> mede o quanto (em porcentagem) da variação da variável dependente pode ser explicado pela introdução do regressor no modelo. Pode-se mostrar que <span class="math inline">\(R^2 \in [0,1]\)</span>.
Expressões alterntivas para <span class="math inline">\(R^2\)</span> são as que segue:
<span class="math display">\[ R^2 = \frac{SQE}{SQT}=1-\frac{SQR}{SQT} =\frac{\sum_i(\widehat{y}_i^*)^2}{\sum_i(y_i^*)^2}=\frac{\sum_{i=1}^n(\widehat{y}_i-\overline{y})^2}{\sum_{i=1}^n(y_i-\overline{y})^2} =1-\frac{\sum_i\widehat{u_i^2}}{\sum_{i=1}^n(y_i-\overline{y})^2},\]</span></p>
<p>Uma deficiência do <span class="math inline">\(R^2\)</span> é que este nunca diminui quando adicionamos regressores, o que implica que o <span class="math inline">\(R^2\)</span> favorece modelos mais complexos. Para minimizar esta deficiência, uma alternativa é penalizar, em certo grau, a inclusão de regressores. Um coeficiente muito utilizado na prática e que faz exatamente isso é o chamado <span class="math inline">\(R^2\)</span> <strong>ajustado</strong> definido por</p>
<p><span class="math display">\[\begin{eqnarray*}
  \overline{R}^2 &amp;=&amp; 1-\frac{[SQR/(n-k-1)]}{[SQT/(n-1)]} \\
   &amp;=&amp; 1- \frac{\sigma^2}{[SQT/(n-1)]},\qquad  \bigg(\sigma^2=\frac{SQR}{n-k-1}\bigg).
\end{eqnarray*}\]</span></p>
<p>O <span class="math inline">\(R^2\)</span> ajustado também recebe o nome de <span class="math inline">\(R^2\)</span> corrigido ou, em inglês, de <span class="math inline">\(R\)</span></p>
<p>Pode-se mostrar que <span class="math inline">\(SQR/(n-k-1)\)</span> é um estimador não-viesado de <span class="math inline">\(\sigma^2\)</span>, a variância populacional do erro, e <span class="math inline">\(SQT/(n-1)\)</span> é um estimador
não-viesado de <span class="math inline">\(\sigma^2_Y\)</span>, a variância de <span class="math inline">\(Y\)</span>.</p>

<div class="proposition">
<span id="prp:prop1" class="proposition"><strong>Proposição 2.1  </strong></span>Se adicionamos um novo regressor à regressão, então <span class="math inline">\(\overline{R}^2\)</span> aumenta e a estatística t deste novo regressor é maior que 1, em módulo.
</div>


<div class="proposition">
<span id="prp:prop2" class="proposition"><strong>Proposição 2.2  </strong></span>Adicionando um grupo de variáveis à regressão, então <span class="math inline">\(\overline{R}^2\)</span> aumenta e a estatística F deste novo grupo de regressores é maior que 1.
</div>

<p>Uma fórmula alternativa para o <span class="math inline">\(\overline{R}^2\)</span> é</p>
<p><span class="math display">\[  \overline{R}^2 = 1-\frac{(1-R^2)(n-1)}{(n-k-1)}.\]</span></p>
<p>Além de permitir a comparação entre modelos ao se incluir/excluir regressores, o
<span class="math inline">\(\overline{R}^2\)</span> serve também para a escolha dentre modelos  (não encaixantes). Por exemplo, o modelo 1 que tem <span class="math inline">\(X_1\)</span>, <span class="math inline">\(X_2\)</span> e <span class="math inline">\(X_3\)</span> como variáveis exlicativas e um outro modelo 2 que tem <span class="math inline">\(X_1\)</span>, <span class="math inline">\(X_2\)</span> e <span class="math inline">\(X_4\)</span>. Mas o <span class="math inline">\(\overline{R}^2\)</span> não serve para escolher dentre formas funcionais diferentes da variável .</p>
</div>
<div id="propriedade-de-não-viés-dos-estimadores-mqo" class="section level3">
<h3><span class="header-section-number">2.6.11</span> Propriedade de não-viés dos estimadores MQO</h3>
<p>Assumindo <span class="math inline">\(X\)</span> não estocástico, tomando a esperança dos estimadores MQO em versão matricial, obtemos:
<span class="math display">\[\begin{eqnarray*}
\mathbb{E}(\hat{\beta})&amp;=&amp;\mathbb{E}[(X&#39;X)^{-1}X&#39;y]=\mathbb{E}[(X&#39;X)^{-1}X&#39;(X\beta+U)]\\
               &amp;=&amp;\mathbb{E}[(X&#39;X)^{-1}X&#39;X\beta]+\mathbb{E}[(X&#39;X)^{-1}X&#39;U]\\
               &amp;=&amp;\beta+(X&#39;X)^{-1}\mathbb{E}[X&#39;U]=\beta,
\end{eqnarray*}\]</span>
pois <span class="math inline">\(\mathbb{E}[X&#39;U]=0\)</span> por hipótese. Ou seja, se as variáveis regressoras são não-correlacionadas com <span class="math inline">\(U\)</span>, o estimador MQO será não-viesado.</p>
<div id="variância-dos-estimadores-mqo" class="section level4">
<h4><span class="header-section-number">2.6.11.1</span> Variância dos estimadores MQO</h4>
<p>Para um modelo de regressão linear múltipla, a variância do estimador de cada <span class="math inline">\(\beta_j\)</span> é dado por</p>
<p><span class="math display">\[\mbox{Var}(\hat{\beta}_j)=
\begin{cases}
\frac{\sigma^2_u}{\mbox{Var}(X_j)}, &amp;\mbox{ se a variância de $U$, $\sigma_U^2$ é conhecida};  \,\,\,\\
  \frac{1}{n-1}\frac{\sum_{i=1}^{n}(\hat{y}_i-\overline{y})^2}{\mbox{Var}(X_j)},&amp; \mbox{se  $\sigma_U^2$ é desconhecida} .
\end{cases} \]</span></p>
</div>
</div>
<div id="testes-de-hipóteses" class="section level3">
<h3><span class="header-section-number">2.6.12</span> Testes de hipóteses</h3>
<div id="teste-t" class="section level4">
<h4><span class="header-section-number">2.6.12.1</span> Teste t</h4>
<p>Se queremos testar individualmente a significância (<span class="math inline">\(H_0: \beta_j=0\)</span>) do
modelo <span class="math display">\[y_i=\beta_0+ \beta_1x_{1i}+\cdots+\beta_kx_{ki} +u_i \]</span>, a estatísticade teste é dada por
<span class="math display">\[t=\frac{\hat{\beta}_j-\beta_j}{\sqrt{\mbox{Var}{\hat{\beta}_j}}}\sim t_{n-k-1}\]</span></p>
<p><br></p>

<div class="remark">
 <span class="remark"><em>Observação. </em></span>  Se houver problema de multicolineariedade, <span class="math inline">\(R^2_j\)</span> será alto, a variância será alta, e a estatística de teste <span class="math inline">\(t\)</span> será baixa, e os estimadores serão pouco significativos (neste caso assumindo <span class="math inline">\(\beta_j=0\)</span>).
</div>

</div>
<div id="teste-f" class="section level4">
<h4><span class="header-section-number">2.6.12.2</span> Teste F</h4>
<p>A estatística <span class="math inline">\(F\)</span> para um modelo com intercepto, que serve para testar se o modelo é significante, ou seja se todos os regressores são conjuntamente significantes, i.e. <span class="math inline">\(H_0: \beta_0=\beta_1=\cdots=\beta_k=0\)</span> vs. <span class="math inline">\(H_1:\mbox{ pelo menos um }\beta_j\neq 0\)</span>,
é dada por
<span class="math display">\[F=\frac{R^2/k}{(1-R^2)/n-k-1}\sim F_{k,n-k-1}.\]</span></p>

<div class="remark">
 <span class="remark"><em>Observação. </em></span> Se temos um problema de multicolineariedade, ainda assim a estatística <span class="math inline">\(F\)</span> e <span class="math inline">\(R^2\)</span> do modelo de <span class="math inline">\(y\)</span> contra <span class="math inline">\(x\)</span> não depende da correlação entre os regressores(apenas do SQR e SQT, ou seja, da soma dos quadrados dos resíduos e da variável dependente) e, assim, se tivermos regressores relevantes para explicar <span class="math inline">\(y\)</span>, então <span class="math inline">\(F\)</span> e <span class="math inline">\(R^2\)</span> indicarão que o modelo como um todo terá um alto poder explicativo.
</div>

</div>
</div>
</div>
<div id="formas-funcionais-logarítmicas" class="section level2">
<h2><span class="header-section-number">2.7</span> Formas funcionais logarítmicas</h2>
<p>Considere o seguinte modelo:</p>
<p><span class="math display">\[\widehat{log\, y}=\hat{\beta}_0+\hat{\beta}_1log\,x_1+\hat{\beta_2}x_2.\]</span></p>
<ul>
<li><p>Ele é log-log de <span class="math inline">\(y\)</span> em relação a <span class="math inline">\(x_1\)</span> e é log-linear em relação a <span class="math inline">\(x_2\)</span>.</p></li>
<li><p><span class="math inline">\(\beta_1\)</span> mede a elasticidade de <span class="math inline">\(y\)</span> em relação a <span class="math inline">\(x_1\)</span>, fixado <span class="math inline">\(x_2\)</span>.</p></li>
<li><p>A interpretação de <span class="math inline">\(\hat{\beta}_1\)</span> é que para o aumento de 1% em <span class="math inline">\(x_1\)</span> temos um aumento de <span class="math inline">\(\beta_1\)</span>% em <span class="math inline">\(y\)</span>.</p></li>
<li><p><span class="math inline">\(\hat{\beta}_2\)</span> pode ser interpretado como: um aumento de uma unidade em <span class="math inline">\(x_2\)</span> dá um aumento exato de <span class="math inline">\(100[\exp{\beta_2}-1]\)</span>% em <span class="math inline">\(y\)</span>.</p></li>
<li><p>Uma medida aproximada, para uma mudança pequena em <span class="math inline">\(x_2\)</span> seria <span class="math inline">\(100\hat{\beta}_2\)</span>%. Este coeficiente é denominado muitas vezes como semi-elasticidade.</p></li>
</ul>
<div style="page-break-after: always;"></div>
</div>
<div id="exercícios" class="section level2">
<h2><span class="header-section-number">2.8</span> Exercícios</h2>

<div class="exercise">
<p><span id="exr:exerint1" class="exercise"><strong>Exercício 2.1  </strong></span>O custo de produção de certo bem é uma variável aleatória com função densidade de probabilidade:
<span class="math display">\[\begin{equation*}
f(x)= kx^2,\,\,\, 1\leq x \leq 4.
\end{equation*}\]</span></p>
<div class="line-block">   a) Calcule o valor de k;</div>
<div class="line-block">   b) Calcule o custo médio do produto;</div>
<div class="line-block">   c) Calcule a probabilidade do custo ser menor do que 2;</div>
<div class="line-block">   d) Calcule a variância do custo do produto;</div>
<div class="line-block">   e) Calcule a probabilidade do custo ser maior do que 3;</div>
</div>

<p><br></p>

<div class="exercise">
<p><span id="exr:exerint2" class="exercise"><strong>Exercício 2.2  </strong></span>Sejam <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span> duas variáveis aleatórias independentes com média <span class="math inline">\(\mu_X=\mathbb{E}(X)=4\)</span>, <span class="math inline">\(\mu_Y=\mathbb{E}(Y)=5\)</span>, <span class="math inline">\(\sigma^2_X=\mbox{Var}(X)=1\)</span> e <span class="math inline">\(\sigma_Y^2=\mbox{Var}(Y)=2.\)</span></p>
<div class="line-block">   a) Calcule <span class="math inline">\(\mathbb{E}(X^2)\)</span> e <span class="math inline">\(\mathbb{E}(Y^2)\)</span>;</div>
<div class="line-block">   b) Calcule <span class="math inline">\(\mbox{Var}(4X-2Y)\)</span>;</div>
<div class="line-block">   c) Calcule <span class="math inline">\(\mbox{Cov}(X,Y)\)</span>;</div>
<div class="line-block">   d) Calcule <span class="math inline">\(\mbox{Cov}(X,2X-3Y)\)</span>;</div>
<div class="line-block">   e) Suponha que <span class="math inline">\(X_1,X_2,\cdots,X_n\)</span> são variáveis aleatórias independentes entre si e independentes de <span class="math inline">\(X\)</span>, mas com a mesma distribuição de probabilidade de <span class="math inline">\(X\)</span>, ou seja, <span class="math inline">\(X_1,X_2,\cdots,X_n\)</span> e <span class="math inline">\(X\)</span> são variáveis aleatórias independentes e identicamente distribuídas (i.i.d) com média <span class="math inline">\(\mu=4\)</span> e variância <span class="math inline">\(\sigma^2=1\)</span>. Calcule:</div>
<p>| i. <span class="math inline">\(\mathbb{E}(\overline{X})=\mathbb{E}\left(\frac{1}{n}\sum_{i=1}^{n}X_i\right);\)</span></p>
<p>| ii. <span class="math inline">\(\mbox{Var}(\overline{X});\)</span></p>
| iii. <span class="math inline">\(\mbox{Cov}(\overline{X},X).\)</span>
</div>

<p><br></p>

<div class="exercise">
<p><span id="exr:exerint3" class="exercise"><strong>Exercício 2.3  </strong></span>Suponha o seguinte modelo linear: <span class="math inline">\(y=X\beta+\mbox{Var}epsilon\)</span>, em que <span class="math inline">\(y\)</span> e <span class="math inline">\(\varepsilon\)</span> são vetores <span class="math inline">\(n\times 1\)</span>, <span class="math inline">\(X&lt;\infty\)</span> é uma
matriz <span class="math inline">\(n\times k\)</span> e <span class="math inline">\(\beta\)</span> é um vetor <span class="math inline">\(k\times 1\)</span>.</p>
<div class="line-block">   a) Determine a(s) hipótese(s) necessária(s) para estimar esse modelo por MQO.</div>
<div class="line-block">   b) Determine a(s) hipótese(s) necessária(s) para que o <span class="math inline">\(\beta\)</span> estimado, <span class="math inline">\(\hat{\beta}\)</span>, exista e seja único.</div>
<div class="line-block">   c) Determine a(s) hipótese(s) necessária(s) para que <span class="math inline">\(\hat{\beta}\)</span> seja não viesado.</div>
<div class="line-block">   d) Determine a(s) hipótese(s) necessária(s) para que <span class="math inline">\(\hat{\beta}\)</span> seja eficiente.</div>
<div class="line-block">   e) Determine a(s) hipótese(s) necessária(s) para que se possa fazer inferência estatística.</div>
</div>

<p><br></p>

<div class="exercise">
<p><span id="exr:exerint4" class="exercise"><strong>Exercício 2.4  </strong></span>Os dados da tabela relacionam o peso de plantas, <span class="math inline">\(Y\)</span> (em gramas) com o percentual de matéria orgânica na terra, <span class="math inline">\(X_1\)</span> e os Kilogramas de nitrogênio suplementares agregados a terra por <span class="math inline">\(1000 m^2\)</span>, <span class="math inline">\(X_2\)</span>:</p>
<div class="line-block">   a) Defina a equação de regressão com intercepto em que <span class="math inline">\(y\)</span> é a variável dependente e <span class="math inline">\(x_1\)</span> e <span class="math inline">\(x_2\)</span> são variáveis explicativas. Não esqueça da suposição para o termo de erro do modelo.</div>
<div class="line-block">   b) Se</div>
<p><span class="math display">\[(X^T X)^{-1}=\left[
\begin{array}{ccc}
1.80  &amp;  -0.07  &amp;  -0.25\\
-0.07 &amp;  0.01   &amp;  -0.00\\
-0.25 &amp;  -0.00   &amp;  0.06 \\
\end{array}
\right],\,\,\,\,\mbox{e}\,\,\,\,\,
X^T Y=\left[
\begin{array}{c}
652.50\\
4915.30\\
3103.66\\
\end{array}
\right],\]</span>
determine <span class="math inline">\(\hat{{\beta}}\)</span> via MQO.</p>
<p>Resposta: <span class="math inline">\(\hat{{\beta}}=(51.56,1.49,6.72)\)</span>.</p>
<div class="line-block">   c) Se <span class="math inline">\(SQ_{res}=27.58\)</span> e <span class="math inline">\(SQ_{total}=28.30\)</span>, calcule o coeficiente de determinação. <strong>Resposta</strong>:<span class="math inline">\(R^2=0.9745\)</span>,</div>
<div class="line-block">   d) Teste <span class="math inline">\(\beta_0=\beta_1=\beta_2=0,\)</span> ou seja, a significância do modelo.</div>
<div class="line-block">   e) Se <span class="math inline">\(dp(\hat{\beta_1})=0.2636\)</span>, (dp=desvio padrão), teste se a variável <span class="math inline">\(X_1\)</span> é relevante para o modelo.</div>
<div class="line-block">   f) Se <span class="math inline">\(dp(\hat{\beta_2})=0.6274\)</span>, teste a hipótese <span class="math inline">\(H_0:\,\,\beta_2=1\)</span>.</div>
</div>

<p><br></p>

<div class="exercise">
<p><span id="exr:exerint5" class="exercise"><strong>Exercício 2.5  </strong></span> Adão Ismiti queria verificar se a produtividade aumentava com a divisão do trabalho. Para isso, fez a seguinte experiência: regrediu a produtividade (<span class="math inline">\(p\)</span>) de <span class="math inline">\(n\)</span> trabalhadores de fábricas de alfinetes contra o número de funções exercidas pelo trabalhador (<span class="math inline">\(F\)</span>), os anos de escolaridade (<span class="math inline">\(E\)</span>), o salário (<span class="math inline">\(w\)</span>) e o número de filhos <span class="math inline">\((N)\)</span>. Formalmente, a regressão
foi:
<span class="math display">\[p_i=\beta_1+\beta_2F_i+\beta_3E_i+\beta_4\omega_i+\beta_5N_i+u_i\]</span></p>
<p>Usando o teste <span class="math inline">\(t\)</span>-Student, Ismiti não rejeitou a hipótese nula de parâmetro igual a zero para <span class="math inline">\(\beta_3\)</span>. Retirou a variável <span class="math inline">\(E\)</span> da
regressão e estimou o modelo restrito, observando que <span class="math inline">\(\hat{\beta_5}\)</span> se tornou também, estatisticamente não significativo. Finalmente, retirou <span class="math inline">\(N\)</span> da regressão e estimou o modelo novamente.</p>
<div class="line-block">   a) Por que não foi preciso fazer o teste F em <span class="math inline">\(\hat{\beta_3}\)</span> para retirar <span class="math inline">\(E\)</span> do modelo?</div>
<div class="line-block">   b) Justifique se o procedimento adotado por Ismiti está correto ou equivocado, para ter eliminado a variável <span class="math inline">\(N\)</span> do modelo.</div>
</div>

<p><br></p>

<div class="exercise">
<p><span id="exr:exerint6" class="exercise"><strong>Exercício 2.6  </strong></span>Suponha um modelo de regressão linear múltiplo em que <span class="math inline">\(\hat{\beta}\)</span> exista, seja não viesado e eficiente, pois <span class="math inline">\(u\)</span> é homocedástico. Suponha que você imponha falsas restrições sobre os parâmetros do modelo.</p>
<div class="line-block">   a) Mostre que as estimativas nesse caso são viesadas.</div>
<div class="line-block">   b) Mostre que a variância das estimativas do modelo com restrições é menor que a variância das estimativas do modelo sem</div>
<p>restrições.</p>
<div class="line-block">   c) Qual é a implicação desse resultado em termos de previsão? Qual é a intuição desse resultado?</div>
Sugestão: Lembre o que é o EQM, ou seja, o erro quadrático médio.
</div>

<p><br></p>

<div class="exercise">
<p><span id="exr:exerint7" class="exercise"><strong>Exercício 2.7  </strong></span>Responda:</p>
<div class="line-block">   a) Cite pelo menos dois testes para a hipótese de homocedasticidade.</div>
<div class="line-block">   b) Cite pelo menos um teste para a hipótese de autocorrelação dos resíduos.</div>
<div class="line-block">   c) Em caso de rejeição da hipótese nula em (a), por qual método você estimaria o modelo?</div>
| d) Em caso de rejeição da hipótese nula em (b), por qual método você estimaria o modelo?
</div>

<p><br></p>

<div class="exercise">
<p><span id="exr:exerint8" class="exercise"><strong>Exercício 2.8  </strong></span>Desafio: Faça os seguinte exercícios.</p>
<div class="line-block">   a) Suponha que <span class="math inline">\(\sum_{i=0}^{\infty}|x_i|&lt;\infty\)</span>. Mostre que <span class="math inline">\(\sum_{i=0}^{\infty}x_i^{2}&lt;\infty\)</span>.</div>
<div class="line-block">   b) Prove (ou não) que <span class="math inline">\(\lim_{n\rightarrow\infty}\sum_{x=1}^{n}\frac{1}{x}=\infty\)</span>.</div>
<div class="line-block">   c) Prove (ou não) que <span class="math inline">\(\lim_{n\rightarrow\infty}\sum_{x=1}^{n}\frac{1}{x^2}=\infty\)</span>.</div>
<div class="line-block">   d) Prove (ou não) que, se <span class="math inline">\(\sum_{i=0}^{\infty}x_i^2&lt;\infty\)</span>, então <span class="math inline">\(\sum_{i=0}^{\infty}|x_i|&lt;\infty\)</span>.</div>
</div>


</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="intro.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="streg.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/01-rev.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Econometria-e-series-temporais-com-o-R.pdf", "Econometria-e-series-temporais-com-o-R.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
